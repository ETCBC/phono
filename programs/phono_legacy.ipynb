{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"left\" src=\"images/laf-fabric-xsmall.png\"/></a>\n",
    "<a href=\"http://etcbc.nl" target=\"_blank\"><img align=\"left\" src=\"images/etcbc-small.png\"/></a>\n",
    "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"left\"src=\"images/etcbc4easy-small.png\"/></a>\n",
    "<a href=\"http://tla.mpi.nl\" target=\"_blank\"><img align=\"right\" src=\"images/TLA-xsmall.png\"/></a>\n",
    "<a href=\"http://www.dans.knaw.nl\" target=\"_blank\"><img align=\"right\"src=\"images/DANS-xsmall.png\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phonetic Transliteration of Hebrew Masoretic Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequently asked questions\n",
    "\n",
    "Q: *What is the use of a phonetic transliteration of the Hebrew Bible? What can anyone wish beyond the careful, meticulous Masoretic system of consonants, vowels and accents?*\n",
    "\n",
    "A: Several things:\n",
    "\n",
    "* the Hebrew Bible may be subject of study in various fields,\n",
    "  where the people involved do not master the Hebrew script;\n",
    "  a phonetic transcription removes a hurdle for them.\n",
    "* in computational linguistics there are many tools that deal with written language in latin alphabets;\n",
    "  even a simple task as getting the consonant-vowel pattern of a word is unnecessarily complicated\n",
    "  when using the Hebrew script.\n",
    "* in phonetics and language learning theory, it is important to represent the sounds without being burdened\n",
    "  by the idiosyncracies of the writing system and the spelling.\n",
    "  \n",
    "Q: *But surely, there already exist transliterations of Hebrew? Why not use them?*\n",
    "\n",
    "Here are a few pragmatic reasons:\n",
    "\n",
    "* we want to be able to *compute* a transliteration based upon our own data;\n",
    "* we want to gain insight in to what extent the transliteration can be purely rule-based, and to what extent\n",
    "  it depends on lexical information that you just need to know;\n",
    "* we want to make available a well documented transliteration, that can be studied, borrowed and improved by others.\n",
    "\n",
    "Q: *But how **good** is your transliteration?*\n",
    "\n",
    "we do not know, ..., yet. A few remarks though:\n",
    "\n",
    "* we have applied most of the *rules* that we could find in Hebrew grammars;\n",
    "* we have suspended some of the rules for some verb paradigms where it is known that they lead to incorrect results\n",
    "* where the rules did not suffice, we have searched the corpus for other occurrences of the same word, to get clues;\n",
    "* where we knew that clues pointed in the wrong direction, we have applied a list of exceptions (currently a list of only the word בָּתִּֽים (\\*bottˈîm => bāttˈîm) \n",
    "* we have a fair test set with critical cases that all pass\n",
    "* we have a few tables of all cases where the algorithm has made corpus based decisions and lexical decisions\n",
    "* we are open for your corrections: login into [SHEBANQ](https://shebanq.ancient-data.org), go to a passage with         offending phonetic transliteration, and make a manual note. **Tip:** Give that note the keyword ``phono``, then we\n",
    "  will collect them.\n",
    "\n",
    "Q: *To me, this is not entirely satisfying.*\n",
    "\n",
    "A: Fair enough. Consider jumping to [Bible Online Learner](http://bibleol.3bmoodle.dk/text/show_text),\n",
    "where they have built in a pretty good transliteration, based on a different method of rule application. It is documented in an article by Nicolai Winther-Nielsen:\n",
    "[Transliteration of Biblical Hebrew for the Role-Lexical Module](http://www.see-j.net/index.php/hiphil/article/view/62) \n",
    "and additional information can be found in Claus Tøndering's\n",
    "[Bible Online Learner, Software on Github](https://github.com/EzerIT/BibleOL).\n",
    "See also [Lex: A software project for linguists](http://www.see-j.net/index.php/hiphil/article/view/60/56).\n",
    "\n",
    "We are planning to conduct an automatic comparison of both transliteration schemes over the whole corpus.\n",
    "\n",
    "Q: *Who is the **we**?*\n",
    "\n",
    "That is the author of this notebook, [Dirk Roorda](mailto:dirk.roorda@dans.knaw.nl), working together with Martijn Naaijer and getting input from Nicolai Winther-Nielsen and Wido van Peursen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the results\n",
    "\n",
    "1. The main result is a python function ``phono(``*etcbc_original*``, ...): ``*phonetic transliteration*.\n",
    "1. Showcases and tests: how the function solves particular classes of problems.\n",
    "   The *cases* file shows a set of cases that have been generated in the last run.\n",
    "   \n",
    "   The *tests* files show a prepared set of cases, against which to test new versions of the algorithm.\n",
    "   1. [mixed](mixed4b.html)\n",
    "      with logfile [mixed_debug](mixed_debug4b.txt).\n",
    "   1. [qamets_nonverb cases](qamets_nonverb_cases4b.html) and [qamets_nonverb tests](qamets_nonverb_tests4b.html)\n",
    "      with logfile [qamets_nonverb_tests_debug](qamets_nonverb_tests_debug4b.txt). \n",
    "      The result of searching the corpus for related occurrences and \n",
    "      having them vote for qatan/gadol interpretation of the qamets.\n",
    "   1. [qamets_verb cases](qamets_verb_cases4b.html) and [qamets_verb tests](qamets_verb_tests4b.html)\n",
    "      with logfile [qamets_verb_tests_debug](qamets_verb_tests_debug4b.txt).\n",
    "      The result of suppressing the qatan interpretation of the qamets regardless of accent\n",
    "      for a definite set of *verb forms*.\n",
    "   1. [qamets_prs cases](qamets_prs_cases4b.html) and [qamets_prs tests](qamets_prs_tests4b.html)\n",
    "      with logfile [qamets_prs_tests_debug](qamets_prs_tests_debug4b.txt).\n",
    "      The result of suppressing the qatan interpretation of the qamets in *pronominal suffixes*.\n",
    "1. A [plain text](combi4b.txt) with the complete text in ETCBC transliteration and phonetic transcription,\n",
    "   verse by verse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the method\n",
    "\n",
    "## Highlevel description\n",
    "\n",
    "1. **ETCBC transliteration**\n",
    "   Our starting point is the ETCBC full transliteration of the Hebrew Masoretic text.\n",
    "   This transliteration is in 1-1 correspondence with the Masoretic text, including all vowels and accents.\n",
    "1. **Grammar rules** \n",
    "   We have implemented the rules we find in grammars of Hebrew about long and short qamets, mobile and silent schwa,\n",
    "   dagesh, and mater lectionis. \n",
    "   The implementation takes the form of a row of *regular expressions*,\n",
    "   where we transliterate targeted pieces of the original.\n",
    "   These regular expressions are exquisitely formulated, and must be applied in the given order.\n",
    "   *Beware:* Seemingly innocent modifications in these expressions or in the order of application,\n",
    "   may ruin the transcription completely.\n",
    "1. **Qamets puzzles: verbs**\n",
    "   In many verb forms the grammar rules would dictate that a certain qamets is qatan while in fact it is gadol.\n",
    "   In most cases this is caused by the fact that no accent has been marked on the syllable that carries the\n",
    "   qamets in question. There is a limited set of verb paradigms where this occurs.\n",
    "   We detect those and suppress qamets qatan interpretation for them.\n",
    "1. **Qamets puzzles: non-verbs**\n",
    "   There are quite a few non-verb occurrences where the accent pattern of a word invites a qamets to become\n",
    "   qatan, that is, by the grammar rules. \n",
    "   Yet, other occurrences of the same lexeme have other accent patterns, and\n",
    "   lead to a gadol interpretation of the same qamets. \n",
    "   In this case we count the unique cases in favour of gadol versus qatan, and let the majority decide for all \n",
    "   occurrences. In cases where we know that the majority votes wrong, we have intervened.\n",
    "   \n",
    "### Qamets work hypothesis\n",
    "Note, that in the the *non-verb qamets puzzles* we have tacitly made the assumption that qamets qatan and gadol are not phonological variants of each other.\n",
    "In other words, it never occurs that a qamets gadol becomes shortened into a qamets qatan.\n",
    "From the grammar rules it follows that short versions of the qamets can only be\n",
    "\n",
    "* patah\n",
    "* schwa\n",
    "* composite schwa with patah\n",
    "\n",
    "and never\n",
    "\n",
    "* qamets qatan\n",
    "* composite schwa with qamets\n",
    "\n",
    "Whether this hypothesis is right, is not my competence. \n",
    "We just use it as a working hypothesis.\n",
    "\n",
    "## Lexical information\n",
    "\n",
    "This method is not a pure method, in the sense that it works only with the information given in the source string.\n",
    "We *cheat*, i.e. we use morphological information from the ETCBC database to \n",
    "steer us into the right directorion. To this end, the input of the ``phono()`` is always a\n",
    "LAF node, from which we can get all information we need.\n",
    "\n",
    "More precisely, it is a sequence of nodes.\n",
    "This sequence is meant to correspond to a sequence of monads, that is written adjacently\n",
    "(no space between, no maqef between).\n",
    "From these nodes we can look up:\n",
    "\n",
    "* the ETCBC transliteration\n",
    "* the qere (if there is a discrepancy between ketiv and qere)\n",
    "* additional lexical information (taken from the last node)\n",
    "\n",
    "## Combined words\n",
    "\n",
    "You can use ``phono()`` to transliterate multiple words at the same time, but you can also do individual words,\n",
    "even if in Hebrew they are written together.\n",
    "However, it is better to feed combined words to ``phono()`` in one go, because the prefix word may influence the transliteration of the postfix word. Think of the article followed by word starting with a BGDKPT letter.\n",
    "The dagesh in the BGDKPT is interpreted as a lene, if the word stands on its own, but as a forte if it is combined.\n",
    "\n",
    "However, it not not advised to feed longer strings to ``phono()``, because when phono retrieves lexical information, it uses the information of the last node that matches a word in the input string.\n",
    "\n",
    "## Accents\n",
    "\n",
    "We determine \"primary\" and \"secundary\" stress in our transliteration, but this must not be taken in a phonetic sense.\n",
    "Every syllable that carries an accent pointing will get a primary stress mark.\n",
    "However, a few specific accent pointings are not deemed to produce an an accent, and an other group of accents\n",
    "is deemed to produce only a secondary accent.\n",
    "The last syllable of a word also gets a secundary accent by default.\n",
    "We have not yet tried to be more precise in this, so *segolates* do not get the treatment they deserve.\n",
    "\n",
    "The main rationale for accents is that they prevent a qamets to be read as qatan.\n",
    "\n",
    "## Individual symbols\n",
    "\n",
    "We have made a careful selection of UNICODE symbols to represent Hebrew sounds.\n",
    "Sometimes we follow the phonetic usage of the symbols, sometimes we follow wide spread custom.\n",
    "The actual mapping can be plugged in quite easily, \n",
    "and the intermediate stages in the transformation do not use these final symbols,\n",
    "so the algorithm can be easily adapted to other choices.\n",
    "\n",
    "### Consonants\n",
    "\n",
    "Provided it is not part of a long vowel, we write yod as ``y``,\n",
    "whilst ``j`` would be more in line with the phonetic alphabet.\n",
    "\n",
    "Likewise, we write ``ו`` as w, if it is not part of a long vowel.\n",
    "\n",
    "With regards to the ``BGDKPT`` letters, it would have been attractive to use the letters ``b g d k p t`` without \n",
    "diacritic for the plosive variants, and with a suitable diacritic for the fricative variants.\n",
    "Alas, the UNICODE table does not offer such a suitable diacritic that is available for all these particular 6 letters.\n",
    "\n",
    "So, we use ``b g d k p t`` for the plosives, but for the fricatives we use ``v ḡ ḏ ḵ f ṯ``.\n",
    "\n",
    "With regards to the *emphatic* consonants ט and ח and צ we represent them with a under dot: ``ṭ ḥ ṣ``.\n",
    "ק is just ``q``.\n",
    "\n",
    "א and ע translate to translate to ``ʕ`` and ``ʔ``.\n",
    "\n",
    "שׁ and שׂ translate to ``š`` and ``ś``.\n",
    "ס is just ``s``.\n",
    "\n",
    "When א and ה are mater lectionis, they are left out. A ה with mappiq becomes just ``h``,\n",
    "like every ה which is not a mater lectionis.\n",
    "\n",
    "We do not mark the deviant final forms of the consonants ך and ם and ן and ף and ץ, assuming that\n",
    "this is just a scriptural peculiarity, with no effect on the actual sounds.\n",
    "\n",
    "The remaining consonants go as follows:\n",
    "\n",
    "<table>\n",
    "<tr><td>ל</td><td>l</td></tr>\n",
    "<tr><td>מ</td><td>m</td></tr>\n",
    "<tr><td>נ</td><td>n</td></tr>\n",
    "<tr><td>ר</td><td>r</td></tr>\n",
    "<tr><td>ז</td><td>z</td></tr>\n",
    "</table>\n",
    "\n",
    "### Vowels\n",
    "\n",
    "The short vowels (patah, segol, hireq) are just ``a e i`` and qibbuts is just ``u``.\n",
    "\n",
    "However, the *furtive* patah is a ``ₐ`` in front of its consonant.\n",
    "\n",
    "The long vowels without yod or waw (qamets gadol, tsere, holam) have an over bar ``ā ē ō``.\n",
    "\n",
    "The complex vowels (tsere or hireq plus yod, holam plus waw, waw with dagesh) have a circumflex ``ê î ô û``.\n",
    "\n",
    "A segol followed by yod becomes ``eʸ``\n",
    "\n",
    "The composite schwas (patah, segol, qamets) are written as superscripts ``ᵃ ᵉ ᵒ``.\n",
    "\n",
    "The simple schwa is left out if silent, and otherwise it becomes ``ᵊ``.\n",
    "\n",
    "### Accent\n",
    "\n",
    "The primary and secundary stress are marked as ``ˈ ˌ`` and are placed *in front of the vowel they occur with*.\n",
    "\n",
    "### Punctuation\n",
    "\n",
    "The sof-pasuq ׃ becomes ``.``. \n",
    "If it is followed by ס (setumah) or ף (petuhah) or  ̇׆ (nun-hafuka), these extra symbols are omitted.\n",
    "\n",
    "The maqef ־ (between words) becomes ``-``.\n",
    "\n",
    "If words are juxtaposed without space in the Hebrew, they are also juxtaposed without space in the phonetic\n",
    "transliteration.\n",
    "\n",
    "### Tetragrammaton\n",
    "\n",
    "The tetragrammaton is transliterated with the vowels it is encountered with, but the whole is put between \n",
    "square brackets ``[ ]``.\n",
    "\n",
    "### Ketiv-qere\n",
    "\n",
    "We base the phonetics on the (vocalized) qere, if a qere is present.\n",
    "The ketiv is then ignored. We precede each such word by a ``*`` to indicate that the qere\n",
    "is deviant from the ketiv. Using the data view it is possible to see what the ketiv is. \n",
    "\n",
    "## Cleaning up\n",
    "\n",
    "We leave the accents and the schwas in the end product of the ``phono()`` function,\n",
    "despite the fact that the accents, as they appear, do not have consistent phonetic significance.\n",
    "And it can be argued that every schwa is silent.\n",
    "If you do not care for schwas and accents, it is easy to remove them.\n",
    "Also, if you find the results in separating the qamets into qatan and gadol unsatisfying or irrelevant, you can\n",
    "just replace them both bij a single symbol, such as ``å``.\n",
    "\n",
    "## Testing\n",
    "\n",
    "Quite a bit of code is dedicated to count special cases, to test, and to produce neat tables with interesting forms.\n",
    "It is also possible to call the ``phono()`` function in debug mode, which will write to a text file all stages in the\n",
    "transliteration from etcbc orginal into the phonetic result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s This is LAF-Fabric 4.8.3\n",
      "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
      "Feature doc: https://shebanq.ancient-data.org/static/docs/featuredoc/texts/welcome.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys, os, collections, re\n",
    "from unicodedata import normalize\n",
    "\n",
    "from laf.fabric import LafFabric\n",
    "from etcbc.preprocess import prep\n",
    "from etcbc.lib import Transcription\n",
    "fabric = LafFabric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the LAF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'version' not in locals(): version = '4b'\n",
    "source = 'etcbc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.01s USING main: etcbc4b DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  6.35s LOGFILE=/Users/dirk/laf/laf-fabric-output/etcbc4b/phono/__log__phono.txt\n",
      "  6.35s INFO: LOADING PREPARED data: please wait ... \n",
      "  6.35s prep prep: G.node_sort\n",
      "  6.41s prep prep: G.node_sort_inv\n",
      "  6.96s prep prep: L.node_up\n",
      "    10s prep prep: L.node_down\n",
      "    16s prep prep: V.verses\n",
      "    16s prep prep: V.books_la\n",
      "    16s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "    16s INFO: LOADED PREPARED data\n",
      "    16s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX  FOR TASK phono AT 2016-12-07T08-59-37\n"
     ]
    }
   ],
   "source": [
    "API = fabric.load('{}{}'.format(source, version), '--', 'phono', {\n",
    "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        otype label\n",
    "        g_word_utf8 g_cons_utf8\n",
    "        g_word g_cons lex_utf8 lex\n",
    "        sp vs vt gn nu ps st\n",
    "        uvf prs g_prs pfm vbs vbe\n",
    "        language\n",
    "        book chapter verse label\n",
    "    ''',''),\n",
    "    \"prepare\": prep(select={'L'}),\n",
    "}, verbose='NORMAL')\n",
    "exec(fabric.localnames.format(var='fabric'))\n",
    "trans = Transcription()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verse index\n",
    "\n",
    "We want to be able to look up the node given a passage and an etcbc transcription string.\n",
    "\n",
    "We also want to be able to easily generate a test from an occurrence that we encounter, whether we have the node, or the transcription string with passage.\n",
    "For that we need a passage index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3.20s Compiling passage index\n",
      "  4.79s 23213 passages (verses)\n"
     ]
    }
   ],
   "source": [
    "msg(\"Compiling passage index\")\n",
    "passage_index = {}\n",
    "vlab2vnode = {}        # old style verse labels needed for reading ketiv qere data\n",
    "\n",
    "for bn in F.otype.s('book'):\n",
    "    book_name = F.book.v(bn)\n",
    "    for cn in L.d('chapter', bn):\n",
    "        chapter_num = F.chapter.v(cn)\n",
    "        for vn in L.d('verse', cn):\n",
    "            verse_num = F.verse.v(vn)\n",
    "            passage_index['{} {}:{}'.format(book_name, chapter_num, verse_num)] = vn\n",
    "            vlab2vnode[F.label.v(vn)] = vn\n",
    "\n",
    "msg('{} passages (verses)'.format(len(passage_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ketiv-qere information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6.46s Reading Ketiv-Qere data\n",
      "  6.48s Read 1892 ketiv-qere annotations\n",
      "  6.53s Parsed 1892 ketiv-qere annotations\n",
      "  6.53s All verses entries found in index\n",
      "  6.53s All ketivs found in the text\n",
      "  6.53s All ketivs found in the data\n"
     ]
    }
   ],
   "source": [
    "kq_file = '{}/kq/kq.{}{}'.format(API['data_dir'], source, version)\n",
    "qeres = {}\n",
    "\n",
    "info = collections.defaultdict(lambda: [])\n",
    "not_found = set()\n",
    "missing = collections.defaultdict(lambda: [])\n",
    "missed = collections.defaultdict(lambda: [])\n",
    "\n",
    "error_limit = 10\n",
    "\n",
    "kq_handle = open(kq_file)\n",
    "\n",
    "ln = 0\n",
    "can = 0\n",
    "cur_label = None\n",
    "\n",
    "msg(\"Reading Ketiv-Qere data\")\n",
    "for line in kq_handle:\n",
    "    ln += 1\n",
    "    can += 1\n",
    "    vlab = line[0:10]\n",
    "    fields = line.rstrip('\\n')[10:].split()\n",
    "    (ketiv, qere) = fields[0:2]\n",
    "    vnode = vlab2vnode.get(vlab, None)\n",
    "    if vnode == None:\n",
    "        not_found.add(vlab)\n",
    "        continue\n",
    "    info[vnode].append((ketiv, qere))        \n",
    "kq_handle.close()\n",
    "msg(\"Read {} ketiv-qere annotations\".format(ln))\n",
    "\n",
    "for vnode in info:\n",
    "    wlookup = collections.defaultdict(lambda: [])\n",
    "    wvisited = collections.defaultdict(lambda: -1)\n",
    "    wnodes = L.d('word', vnode)\n",
    "    for w in wnodes:\n",
    "        gw = F.g_word.v(w)\n",
    "        if '*' in gw:\n",
    "            gw = F.g_cons.v(w)\n",
    "            if gw == '': gw = '.'\n",
    "            wlookup[gw].append(w)\n",
    "    for (ketiv, qere) in info[vnode]:\n",
    "        wvisited[ketiv] += 1\n",
    "        windex = wvisited[ketiv]\n",
    "        ws = wlookup.get(ketiv, None)\n",
    "        if ws == None or windex > len(ws) - 1:\n",
    "            missing[vnode].append((windex, ketiv, qere))\n",
    "            continue\n",
    "        w = ws[windex]\n",
    "        qeres[w] = qere\n",
    "    for ketiv in wlookup:\n",
    "        if ketiv not in wvisited or len(wlookup[ketiv]) - 1 > wvisited[ketiv]:\n",
    "            missed[vnode].append((len(wlookup[ketiv]) - (wvisited.get(ketiv, -1) + 1), ketiv))\n",
    "msg(\"Parsed {} ketiv-qere annotations\".format(len(qeres)))\n",
    "\n",
    "if not_found:\n",
    "    msg(\"Could not find {} verses: {}\".format(len(not_found), sorted(not_found)))\n",
    "else:\n",
    "    msg(\"All verses entries found in index\")\n",
    "if missing:\n",
    "    msg(\"Could not locate ketivs in the text: {} verses\".format(len(missing)))\n",
    "    e = 0\n",
    "    for vnode in sorted(missing):\n",
    "        if e > error_limit: break\n",
    "        vlab = F.label.v(vnode)\n",
    "        for (windex, ketiv, qere) in missing[vnode]:\n",
    "            e += 1\n",
    "            if e > error_limit: break\n",
    "            print('NOT IN TEXT: {:<10} {:<20} #{} {}'.format(vlab, ketiv, windex, qere))\n",
    "else:\n",
    "    msg(\"All ketivs found in the text\")\n",
    "if missed:\n",
    "    msg(\"Could not lookup qeres in the data: {} verses\".format(len(missing)))\n",
    "    e = 0\n",
    "    for vnode in sorted(missed):\n",
    "        if e > error_limit: break\n",
    "        vlab = F.label.v(vnode)\n",
    "        for (windex, ketiv) in missed[vnode]:\n",
    "            e += 1\n",
    "            if e > error_limit: break\n",
    "            print('NOT IN DATA: {:<10} {:<20} #{}'.format(vlab, ketiv, windex))\n",
    "else:\n",
    "    msg(\"All ketivs found in the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The source string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what we use as our starting point: the etcbc transliteration, with one or two tweaks.\n",
    "\n",
    "The ETCBC transcription encodes also what comes after each word until the next word.\n",
    "Sometimes we want that extra bit, and sometimes not, and sometimes part of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuation\n",
    "punctuation = re.compile('''\n",
    "      (?: [ -]\\s*\\Z)        # space, (no maqef) or nospace\n",
    "    | (?: \n",
    "           0[05]            # sof pasuq or paseq\n",
    "           (?:_[SNP])*      # nun hafukha, setumah, petuhah at end of verse\n",
    "           \\s*\\Z\n",
    "      )\n",
    "    | (?:_[SPN]\\s*\\Z)       #  nun hafukha, setumah, petuhah between words\n",
    "''', re.X)\n",
    "\n",
    "split_punctuation = re.compile('''\n",
    "  (.*?)                 # part before punctuation\n",
    "  ((?:                  # punctuation itself\n",
    "      (?: [ &-]\\s*)         # space, maqef, or nospace\n",
    "    | (?: \n",
    "           0[05]            # sof pasuq or paseq\n",
    "           (?:_[SNP])*      # nun hafukha, setumah, petuhah at end of verse\n",
    "           \\s*\n",
    "      )\n",
    "    | (?:_[SPN]\\s*)         #  nun hafukha, setumah, petuhah between words\n",
    "  )*)\n",
    "''', re.X)\n",
    "\n",
    "start_punct = re.compile('''\n",
    "      (?: \\A[ &-]\\s*)       # space, maqef or nospace\n",
    "    | (?: \n",
    "           \\A\n",
    "           0[05]            # sof pasuq or paseq\n",
    "           (?:_[SNP])*      # nun hafukha, setumah, petuhah at end of verse\n",
    "           \\s*\n",
    "      )\n",
    "    | (?:\\A\\s*_[SPN]\\s*)    #  nun hafukha, setumah, petuhah between words\n",
    "''', re.X)\n",
    "\n",
    "noorigspace = re.compile('''\n",
    "      (?: [&-]\\Z)           # space, maqef or nospace\n",
    "    | (?: \n",
    "           0[05]            # sof pasuq or paseq\n",
    "           (?:_[SNP])*      # nun hafukha, setumah, petuhah at end of verse\n",
    "           \\Z\n",
    "      )\n",
    "    | (?:_[SPN])+           #  nun hafukha, setumah, petuhah between words\n",
    "''', re.X)\n",
    "\n",
    "# setumah and petuhah\n",
    "# Usually, setumah and petuhah occurr after the end of verse sign.\n",
    "# In that case we can strip them.\n",
    "# Sometimes they occur interword. Then we have to replace them by a space\n",
    "# because the words are otherwise adjacent.\n",
    "# This operation must be performed before originals are glued together,\n",
    "# because the _S and _P can only be reliably detected if they are at the end of a word.\n",
    "# So: set_pet to be used before phono(), in get_orig, but only if get_orig is\n",
    "# used for phono(). \n",
    "\n",
    "set_pet_pattern = re.compile('((?:0[05])?)(_[SNP])+\\Z')\n",
    "tetra_lex = 'JHWH/'\n",
    "\n",
    "def set_pet_pattern_repl(match):\n",
    "    (punct, nsp) = match.groups()\n",
    "    sep = ' § ' if punct == '' and nsp != '' else ''\n",
    "    return punct+sep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_orig(w, punct=True, set_pet=False, tetra=True, give_ketiv=False):\n",
    "    proto = F.g_word.v(w)\n",
    "    orig =  proto if give_ketiv else qeres.get(w, proto)\n",
    "    if tetra and F.lex.v(w) == tetra_lex:\n",
    "        (mat, sep) = split_punctuation.fullmatch(orig).groups()\n",
    "        orig = '[ '+mat+' ]'+sep\n",
    "    if not punct:\n",
    "        orig = punctuation.sub('', orig)\n",
    "    else:\n",
    "        if not noorigspace.search(orig):\n",
    "            orig += ' '\n",
    "        if not set_pet:\n",
    "            orig = set_pet_pattern.sub(set_pet_pattern_repl, orig)\n",
    "    return orig\n",
    "\n",
    "# find the first occurrence of the string orig in the verse (ETCBC representation)\n",
    "# Then deliver the sequence of nodes corresponding to that sequence\n",
    "# it turns out that too much is happening with accents, so I will \"normalize\" the accents for the\n",
    "# sake of looking up\n",
    "\n",
    "digit = re.compile('[0-9]+')\n",
    "\n",
    "def find_w(passage, orig, debug=False):\n",
    "    if len(orig) == 0: return None\n",
    "    vn = passage_index.get(passage, None)\n",
    "    if vn == None: return None\n",
    "    verse_words = L.d('word', vn)\n",
    "    results = None\n",
    "    orig = orig.strip()+' '\n",
    "    lvw = len(verse_words)\n",
    "    for i in range(lvw):\n",
    "        target = orig\n",
    "        for j in range(i, lvw+1):\n",
    "            target = start_punct.sub('', target)\n",
    "            target = digit.sub('', target)\n",
    "            if len(target) == 0:\n",
    "                results = verse_words[i:j]\n",
    "                break\n",
    "            if j >= lvw:\n",
    "                break\n",
    "            j_orig = digit.sub('', get_orig(\n",
    "                verse_words[j], \n",
    "                punct=False, tetra=False, give_ketiv=True,\n",
    "            )).rstrip('&')\n",
    "            if target.startswith(j_orig):\n",
    "                if debug: print('{}-{}: [{}] <= [{}]'.format(i, j, j_orig, target))\n",
    "                target = target[len(j_orig):]\n",
    "                if debug: print('{}-{}: [{}]'.format(i, j, target))\n",
    "                continue\n",
    "            if debug: print('{}-{}: [{}] <! [{}]'.format(i, j, j_orig, target))\n",
    "            break\n",
    "    return results\n",
    "\n",
    "# partition a list of nodes into chunks \n",
    "# whenever a node has an orig string that not ends with an - start a new chunk\n",
    "def partition_w(wnodes):\n",
    "    results = []\n",
    "    cur_chunk = []\n",
    "    orig = None\n",
    "    for w in wnodes:\n",
    "        cur_chunk.append(w)\n",
    "        orig = get_orig(w, tetra=False)\n",
    "        if orig.endswith('-'): continue\n",
    "        results.append(tuple(cur_chunk))\n",
    "        cur_chunk = []\n",
    "    if len(cur_chunk):\n",
    "        results.append(tuple(cur_chunk))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The phonological symbols\n",
    "\n",
    "Here is the list of symbols that constitutes the mapping from ETCBC transcription codes to a phonetic transcription.\n",
    "It is a series of triplets (*etcbc symbol*, *name*, *phonetic symbol*).\n",
    "\n",
    "If changes are needed to the appearance of the phonetic transcriptions (not to its *logic*), here is the place to tweak.\n",
    "\n",
    "Note that the order is important.\n",
    "In the final stage of the transformation process, these substitutions will be applied in the order they appear here.\n",
    "\n",
    "This is especially important for, but not only for, the BGDKPT letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "specials = (\n",
    "    ('>', 'alef', 'ʔ'),\n",
    "    ('<', 'ayin', 'ʕ'),\n",
    "    ('v', 'tet', 'ṭ'),\n",
    "    ('y', 'tsade', 'ṣ'),\n",
    "    ('x', 'chet', 'ḥ'),\n",
    "    ('c', 'shin', 'š'),\n",
    "    ('f', 'sin', 'ś'),\n",
    "    ('#', 's(h)in', 'ŝ'),\n",
    "\n",
    "    ('ij', 'long hireq', 'î'),\n",
    "    ('I', 'short hireq', 'i'),\n",
    "    (';j', 'long tsere', 'ê'),\n",
    "    ('ow', 'long holam', 'ô'),\n",
    "    ('w.', 'long `qibbuts`', 'û'),\n",
    "    ('ej', 'e glide', 'eʸ'),\n",
    "    ('j', 'yod', 'y'),\n",
    "\n",
    "    (':a', 'hataf patach', 'ᵃ'),\n",
    "    (':@', 'hataf qamats', 'ᵒ'),\n",
    "    (':e', 'hataf segol', 'ᵉ'),\n",
    "    ('%', 'schwa mobile', 'ᵊ'),\n",
    "    (':', 'schwa quiescens', ''),\n",
    "    ('@', 'qamats gadol', 'ā'),\n",
    "    ('a', 'patach', 'a'),\n",
    "    ('`', 'furtive patach', 'ₐ'),\n",
    "    ('+', 'qamats', 'å'),\n",
    "    ('e', 'segol', 'e'),\n",
    "    (';', 'tsere', 'ē',),\n",
    "    ('i', 'hireq', 'i'),\n",
    "    ('o', 'holam', 'ō'),\n",
    "    ('^', 'qamats qatan', 'o'),\n",
    "    ('u', 'qibbuts', 'u'),\n",
    "\n",
    "    ('b.', 'b plosive', 'B'),\n",
    "    ('g.', 'g plosive', 'G'),\n",
    "    ('d.', 'd plosive', 'D'),\n",
    "    ('k.', 'k plosive', 'K'),\n",
    "    ('p.', 'p plosive', 'P'),\n",
    "    ('t.', 't plosive', 'T'),\n",
    "\n",
    "    ('b', 'b fricative', 'v'),\n",
    "    ('g', 'g fricative', 'ḡ'),\n",
    "    ('d', 'd fricative', 'ḏ'),\n",
    "    ('k', 'k fricative', 'ḵ'),\n",
    "    ('p', 'p fricative', 'f'),\n",
    "    ('t', 't fricative', 'ṯ'),\n",
    "\n",
    "    ('B', 'b plosive', 'b'),\n",
    "    ('G', 'g plosive', 'g'),\n",
    "    ('D', 'd plosive', 'd'),\n",
    "    ('K', 'k plosive', 'k'),\n",
    "    ('P', 'p plosive', 'p'),\n",
    "    ('T', 't plosive', 't'),\n",
    "    \n",
    "    ('w', 'waw', 'w'),\n",
    "    ('l', 'lamed', 'l'),\n",
    "    ('m', 'mem', 'm'),\n",
    "    ('n', 'nun', 'n'),\n",
    "    ('r', 'resh', 'r'),\n",
    "    ('z', 'zajin', 'z'),\n",
    "    \n",
    "    ('!', 'primary accent', 'ˈ'),\n",
    "    ('/', 'secundary accent', 'ˌ'),\n",
    "    \n",
    "    ('&', 'maqef', '-'),\n",
    "    ('*', 'masora', '*'),\n",
    ")\n",
    "\n",
    "specials2 = (\n",
    "    ('$', 'sof pasuq', '.'),\n",
    "    ('|', 'paseq', ' '),\n",
    "    ('§', 'interword setumah and petuhah', ' '),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembling the symbols in dictionaries\n",
    "\n",
    "We compile the table of symbols in handy dictionaries for ease of processing later.\n",
    "\n",
    "We need to quickly detect the dagesh lenes later on, so we store them in a dictionary.\n",
    "\n",
    "Our treatment of accents is still primitive. \n",
    "\n",
    "We ignore some accents (``irrelevant accents`` below) and we consifer some accents as indicators of a mere\n",
    "*secundary* accent (``secundary accents`` below).\n",
    "\n",
    "The ``sound_dict`` is the resultig (ordered) mapping of all source characters to \"phonetic\" characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dagesh_lenes = {'b.', 'g.', 'd.', 'k.', 'p.', 't.'}\n",
    "dagesh_lene_dict = dict()\n",
    "\n",
    "irrelevant_accents = (\n",
    "    ('01', 'segol'),  # occurs always with another accent\n",
    "    ('03', 'pashta'), # by definition on last syllable: not relevant for accent\n",
    "    ('04', 'telisha qetana'),\n",
    "    ('14', 'telisha gedola'),\n",
    "    ('24', 'telisha qetana'),\n",
    "    ('44', 'telisha gedola'),\n",
    ")\n",
    "secundary_accents = (\n",
    "    ('71', 'merkha'), # ??\n",
    "    ('63', 'qadma'),  # ??\n",
    "    ('73', 'tipeha'), # ??\n",
    ")\n",
    "punctuation_accents = (\n",
    "    ('00', 'sof pasuq'),\n",
    "    ('05', 'paseq'),\n",
    ")\n",
    "\n",
    "known_accents = {x[0] for x in irrelevant_accents+secundary_accents+punctuation_accents}\n",
    "\n",
    "primary_accents = (\n",
    "    {'{:>02}'.format(i) for i in range(100) if '{:>02}'.format(i) not in known_accents}\n",
    ")\n",
    "sound_dict = collections.OrderedDict() \n",
    "sound_dict2 = collections.OrderedDict() \n",
    "\n",
    "\n",
    "for (sym, let, glyph) in specials:\n",
    "    if sym in dagesh_lenes:\n",
    "        dagesh_lene_dict[sym[0]] = glyph\n",
    "    else:\n",
    "        sound_dict[sym] = glyph\n",
    "        \n",
    "for (sym, let, glyph) in specials2:\n",
    "    sound_dict2[sym] = glyph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patterns\n",
    "\n",
    "The ``phono()`` function that we will define (far) below, performs an ordered sequence of transformations.\n",
    "Most of these are defined as [regular expressions](http://www.regular-expressions.info),\n",
    "and some parts of those expressions occur over and over again, e.g. subpatterns for *vowel* and *consonant*.\n",
    "\n",
    "Here we define the shortcuts that we are going to use in the regular expressions.\n",
    "\n",
    "## Details of the matching process\n",
    "\n",
    "Normally, when a pattern matches a string, the string is consumed: the parts of the pattern that match\n",
    "consume corresponding stretches of the string.\n",
    "However, in many cases a pattern specifies specific contexts in which a match should be found.\n",
    "In those cases we do not want that the context parts of the pattern are responsible for string\n",
    "consumption, because in those parts there could be another relevangt match.\n",
    "\n",
    "In regular expression there is a solution for that: look-ahead and look-behind assertions and we use them frequently.\n",
    "\n",
    "``(?<=`` *before_pattern* ``)`` *pattern* ``(?=`` *behind-pattern* ``)``\n",
    "\n",
    "A match of this pattern in a string is a portion of a string that matches *pattern*, provided that\n",
    "portion is preceded by *before_pattern* and followed by *behind* pattern.\n",
    "\n",
    "If there is a match, and new matches must be searched for, the search will start right after *pattern*.\n",
    "\n",
    "Instead of the above *positive* look-ahead and look-behind assertions, there are also *negative* variants:\n",
    "\n",
    "``(?<!`` *before_pattern* ``)`` *pattern* ``(?!`` *behind-pattern* ``)``\n",
    "\n",
    "in those cases the match is good, if the *before_pattern* does not match the preceding material, and analogously\n",
    "the *behind_pattern*.\n",
    "\n",
    "In Python there is a restriction on look-behind patterns:\n",
    "they must be patterns that only have matches of a predictable, fixed length.\n",
    "That will make some of our patterns slightly more complicated.\n",
    "For example, vowels can be simple or complex, and hence have variable length.\n",
    "If we want to specify a consonant, provided it is preceded by a vowel, we have to be careful.\n",
    "\n",
    "In regular expressions there are *greedy*, *non-greedy* and *possessive* quantifiers. \n",
    "Greedy ones try to match as many times as possible at first;\n",
    "non-greedy ones try to match as few times as possible at first.\n",
    "Possessive quantifiers are like greedy ones, but greedy ones will give back occurrences if that helps \n",
    "to achieve a match. Possessive ones do not do that.\n",
    "\n",
    "<table>\n",
    "<tr><th>kind</th><th>greedy</th><th>non-greedy</th><th>possessive</th></tr>\n",
    "<tr><th>0 or more</th><td>``*``</td><td>``*?``</td><td>``*+``</td></tr>\n",
    "<tr><th>1 or more</th><td>``+``</td><td>``+?``</td><td>``++``</td></tr>\n",
    "<tr><th>at least *n*, at most *m*</th>\n",
    "    <td>``{``*n*``,`` *m*``}``</td>\n",
    "    <td>``{``*n*``,`` *m*``}?``</td>\n",
    "    <td>``{``*n*``,`` *m*``}+``</td>\n",
    " </tr>\n",
    "</table>\n",
    "\n",
    "For example, the pattern ``[ab]*b`` matches substrings of ``a``s and ``b``s that end in a ``b``.\n",
    "In order to match the string ``aaaaab``, the ``[a|b]*`` part starts with greedily consuming the whole string,\n",
    "but after discovering that the ``b`` part in the pattern should also match something, the ``[a|b]*`` part\n",
    "reluctantly gives back one occurrence. That will do the trick.\n",
    "\n",
    "However, ``[ab]*+b`` will not match ``aaaaab``, because the possessive quantifier gives nothing back.\n",
    "\n",
    "Possessive quantifiers a desirable in combination with negative look-behind assertions.\n",
    "\n",
    "For example, take ``[ab]*+(?!c)$``. This will match substrings of ``a``s and ``b``s that are not followed by ``c``.\n",
    "So it matches ``ababab`` but not ``abababc``.\n",
    "However, the non-possessive variant, ``[ab]*(?!c)`` matches both. So how does it match ``abababc``?\n",
    "First, the ``[ab]*`` part matches all ``a``s and ``b``s. Then the look-behind assertion that ``c`` does not follow,\n",
    "is violated. So ``[ab]*`` backtracks one occurrence, a ``b``. At that point the look-behind assertion finds a ``b`` \n",
    "which is not ``c``, and the match succeeds.\n",
    "\n",
    "Python lacks *possessive* quantifiers in regular expressions, so again, this makes some expressions below more complicated than they were otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to test for vowels in look-behind conditions.\n",
    "# Python insists that look-behind conditions match patterns with fixed length.\n",
    "# Vowels have variable length, so we need to take a bit more context.\n",
    "# This extra context is dependent on whether the vowel occurs in front of a consonant or after it\n",
    "# vowel1 is for before, vowel2 is for after, both are usable in look-behind conditions\n",
    "# vowel matches purely vowels of variable length, and is not usable in look-behind conditions\n",
    "\n",
    "vowel1 = '(?:(?::[ea@])|(?:w\\.)|(?:[i;]j)|(?:ow)|(?:.[%@\\^;aeiIou`]))'\n",
    "vowel2 = '(?:(?::[ea@])|(?:w\\.)|(?:[i;]j)|(?:ow)|(?:[%@\\^;aeiIou`].))'\n",
    "vowel = '(?:(?::[ea@])|(?:w\\.)|(?:[i;]j)|(?:ow)|(?:[%@\\^;aeiIou`]))'\n",
    "\n",
    "# lvowel are long vowels only (including compositions)\n",
    "# svowel are short vowels only, including composite schwas\n",
    "lvowel1 = '(?:(?:w\\.)|(?:[i;]j)|(?:ow)|(?:.[@;o]))'\n",
    "svowel = '(?:(?::[ea@])|(?:[%@\\^;aeiIou`]))'\n",
    "\n",
    "gadol = sound_dict['@']\n",
    "qatan = sound_dict['^']\n",
    "a_like = {':a', 'a'}\n",
    "o_like = {':@', 'o', 'ow', 'u', 'w.'}\n",
    "e_like = {':', ':e', ';', ';j', 'e', 'i', 'ij'}\n",
    "\n",
    "# complex i/w vowel: the composite vowels with waw and yod, after translation\n",
    "complex_i_vowel = ''.join(sound_dict[s] for s in {'ij', ';j'})\n",
    "complex_w_vowel = ''.join(sound_dict[s] for s in {'ow'})\n",
    "\n",
    "# consonants\n",
    "ncons = '[^>bgdhwzxvjklmns<pyqrfct _&$-]' # not a consonant\n",
    "cons = '[>bgdhwzxvjklmns<pyqrfct]'        # any consonant\n",
    "consx = '[bgdwzxvjklmns<pyqrfct]'         # any consonant except alef\n",
    "bgdkpt = '[bgdkpt]'                       # begadkefat consonant\n",
    "nbgdkpt = '[wzxvjlmns<yqrfc]'             # non-begadkefat consonant\n",
    "prep = '[bkl]'                            # proclitic preposition\n",
    "\n",
    "# accents\n",
    "\n",
    "acc = '[ˈˌ]'                              # primary and secundary accent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular expressions\n",
    "\n",
    "Here are the patterns, but also the replacement functions we are going to carry out when the patterns match.\n",
    "How exactly the patterns and replacement functions hang together, is a matter for the phono function itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rafe and furtive patah\n",
    "\n",
    "### Rafe\n",
    "\n",
    "The rafe indicates a fricative pronounciation. It cancels a dagesh lene on a BGDKPT letter.\n",
    "If it occurs in other situations, we ignore it.\n",
    "\n",
    "### Furtive patah\n",
    "\n",
    "We have to reverse any CV pattern at word ends where the V is a patah, and the C is a guttural (i.e. cheth, ayin or he-mappiq).\n",
    "\n",
    "If there is an accent on the guttural, we ignore it in these cases, because the guttural does not initiate a syllable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rafe\n",
    "\n",
    "rafe = re.compile('({b})\\.,'.format(b=bgdkpt))\n",
    "\n",
    "def rafe_repl(match):\n",
    "    return match.group(1)\n",
    "\n",
    "# furtive patah\n",
    "# note that we will deliberately loose any accent on the guttural\n",
    "furtive_patah = re.compile('([x<]|(?:h\\.))(?:[/!]?)a(?=\\Z|[ &-])'.format(v1=vowel1))\n",
    "\n",
    "def furtive_patah_repl(match):\n",
    "    return '`'+match.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accents\n",
    "\n",
    "### Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicit accents\n",
    "\n",
    "# lets assume that any cantillation mark or accent indicates that the vowel is stressed\n",
    "# except for some types of mark (qadma, pashta)\n",
    "sep_accent = re.compile('([0-9]{2})')\n",
    "remove_accent = re.compile('|'.join('~{}'.format(x[0]) for x in irrelevant_accents))\n",
    "primary_accent = re.compile('|'.join('~{}'.format(x) for x in primary_accents))\n",
    "secundary_accent = re.compile('|'.join('~{}'.format(x[0]) for x in secundary_accents))\n",
    "punctuation_accent = re.compile('({})'.format('|'.join('~{}'.format(x[0]) for x in punctuation_accents)))\n",
    "condense_accents = re.compile('({v})([!/]+)'.format(v=vowel))\n",
    "\n",
    "def sep_accent_repl(match):\n",
    "    return '~'+match.group(1)\n",
    "\n",
    "def condense_accents_repl(match):\n",
    "    accent = '!' if '!' in match.group(2) else '/'\n",
    "    return accent+match.group(1)\n",
    "\n",
    "# implicit accents\n",
    "last_part = re.compile('([^&-]*)\\Z')\n",
    "default_accent1 = re.compile('({v}`?{c}?\\.?(?:\\Z|[ ]))'.format(v=svowel, c=cons))\n",
    "default_accent2 = re.compile('({v}(?:\\Z|[ ]))'.format(v=lvowel1))\n",
    "strip_accents = re.compile('[0-9*]')\n",
    "\n",
    "# wrong last accents\n",
    "last_accent = re.compile('[/!]+(?=[ ]|\\Z)')\n",
    "\n",
    "def default_accent_repl(match):\n",
    "    return '/'+match.group(1)\n",
    "\n",
    "def punctuation_accent_repl(match):\n",
    "    if match.group(1) == '~00': return ' $'\n",
    "    return ' | '\n",
    "\n",
    "# separate the phonetic representation from the interword material after it.\n",
    "# To be used at the end of phono().\n",
    "# specials2 specify how punctuation (sof pasuq, paseq, interword setumah-petuhah are\n",
    "# translated).\n",
    "\n",
    "phono_sep = re.compile('(.*?)([ {}]*)'.format(''.join(x[2] for x in specials2)))\n",
    "multiple_space = re.compile('  +')\n",
    "\n",
    "verse_end_phono = re.compile('(\\. *)\\Z')\n",
    "\n",
    "def verse_end_phono_repl(match):\n",
    "    return match.group(1).replace(' ', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats = collections.Counter()\n",
    "\n",
    "def doaccents(orig, debug=False, count=False):\n",
    "    dout = []\n",
    "\n",
    "# prepare\n",
    "    if debug: dout.append(('orig', orig))\n",
    "    if count: pre = orig\n",
    "    result = orig.lower().replace('_', ' ')\n",
    "    if debug: dout.append(('trim', result))\n",
    "    if count and pre != result: stats['trim'] += 1\n",
    "\n",
    "# explicit accents\n",
    "    if count: pre = result\n",
    "    result = sep_accent.sub(sep_accent_repl, result)\n",
    "    result = remove_accent.sub('', result)\n",
    "    result = secundary_accent.sub('/', result)\n",
    "    result = primary_accent.sub('!', result)\n",
    "    result = condense_accents.sub(condense_accents_repl, result)\n",
    "    if debug: dout.append(('accents', result))\n",
    "    if count and pre != result: stats['accents'] += 1\n",
    "\n",
    "# punctuation\n",
    "    if count: pre = result\n",
    "    result = punctuation_accent.sub(punctuation_accent_repl, result)\n",
    "    result = strip_accents.sub('', result)\n",
    "    if debug: dout.append(('punctuation', result))\n",
    "    if count and pre != result: stats['punctuation'] += 1\n",
    "\n",
    "# rafe\n",
    "    if count: pre = result\n",
    "    result = rafe.sub(rafe_repl, result)\n",
    "    result = result.replace(',', '')\n",
    "    if debug: dout.append(('rafe', result))\n",
    "    if count and pre != result: stats['rafe'] += 1\n",
    "\n",
    "# furtive patah\n",
    "    if count: pre = result\n",
    "    result = furtive_patah.sub(furtive_patah_repl, result)    \n",
    "    if debug: dout.append(('furtive_patah', result))\n",
    "    if count and pre != result: stats['furtive_patah'] += 1\n",
    "\n",
    "# implicit accents\n",
    "    if count: pre = result\n",
    "    hotpart = last_part.search(result).group(1) \n",
    "    if '!' not in hotpart and '/' not in hotpart:    \n",
    "        result = default_accent1.sub(default_accent_repl, result)\n",
    "        if not '/' in result:\n",
    "            result = default_accent2.sub(default_accent_repl, result)\n",
    "    result = last_accent.sub('', result)\n",
    "    if debug: dout.append(('default accent', result))\n",
    "    if count and pre != result: stats['default_accent'] += 1\n",
    "\n",
    "# deliver\n",
    "    return (result, dout) if debug else result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qamets gadol and qatan\n",
    "\n",
    "### Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# qamets qatan  \n",
    "# NB: all patterns stipulate that the qamets (@) in question is unaccented\n",
    "\n",
    " # near end of word:\n",
    "qamets_qatan1 = re.compile('(?<={c})(\\.?)@(?={c}(?:\\.?[/!]?(?:[ &-]|\\Z)))'.format(c=consx))\n",
    "\n",
    "# before dagesh forte:\n",
    "qamets_qatan2 = re.compile('(?<={c})(\\.?)@(?={c}\\.)'.format(c=cons))\n",
    "\n",
    "# if the following consonant is BGDKFT and does not have dagesh, the @ is in an open syllable:\n",
    "qamets_qatan3 = re.compile('(?<={c})(\\.?)@(?={c}:(?:{nb}|(?:{b}\\.)))'.format(c=cons, b=bgdkpt, nb=nbgdkpt))\n",
    "\n",
    "# assimilation of qamets with following composite schwa of type (chatef qamets),\n",
    "#     but if the qamets is under a preposition BCL, not if it is under the article H:\n",
    "qamets_qatan4a = re.compile('(?<={p})(\\.?[!/]?)@(?=-{c}:@)'.format(p=prep, c=cons))\n",
    "\n",
    "#     or word-internal\n",
    "qamets_qatan4b = re.compile('(?<={c})(\\.?[!/]?)@(?={c}:@)'.format(c=cons))\n",
    "\n",
    "# before an other qamets qatan, provided the syllable is unaccented\n",
    "qamets_qatan5 = re.compile('(?<={c})(\\.?)@(?={c}\\.?[/!]?\\^)'.format(c=cons))\n",
    "\n",
    "# in a pronominal suffix, qamets never becomes qatan.\n",
    "# This pattern will be applied only on words that do have a non-empty pronominal suffix\n",
    "# The pattern will spot the qamets qatan in front of the last consonant, if there is such a qatan\n",
    "\n",
    "qamets_qatan_prs = re.compile('\\^(?=[0-9]*{c}\\.?[/!]?(?:[ &-]|\\Z))'.format(c=cons))\n",
    "\n",
    "def qamets_qatan_repl(match):\n",
    "    return match.group(1)+'^'\n",
    "\n",
    "# there are exceptions to the heuristic of interpreting qamets by voting between occurrences\n",
    "qamets_qatan_x = '''\n",
    "BJT/ => 1A\n",
    "JM/ => 1O\n",
    "JWMM => 2A\n",
    "JRB<M/ => 1A\n",
    "JHWNTN/ => 2A\n",
    "'''\n",
    "\n",
    "xxx = '''\n",
    "<YBT/ => 2A\n",
    "'''\n",
    "\n",
    "# there are unaccented conjugated verb forms that must not be subjected to qamets-qatan transformation\n",
    "qamets_qatan_verb_x = {\n",
    "    'verb qal perf 3sf',\n",
    "    'verb qal perf 3p-',\n",
    "    'verb nif impf 1s-',\n",
    "    'verb nif impf 1p-',\n",
    "    'verb nif impf 2sf',\n",
    "    'verb nif impf 2pm',\n",
    "    'verb nif impf 3pm',\n",
    "    'verb nif impv 2sf',\n",
    "    'verb nif impv 2pm',\n",
    "}\n",
    "qqv_experimental = {\n",
    "    'verb qal impf 3pm',\n",
    "}\n",
    "\n",
    "qamets_qatan_verb_x |= qqv_experimental\n",
    "\n",
    "def qamets_qatan_verb_x_repl(match):\n",
    "    return match.group(1)+'@'\n",
    "# for the use of applying individual corrections:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions\n",
    "Here is the function that carries out rule based qamets qatan detection, without going into\n",
    "verb paradigms and exceptions. It is the first go at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doplainqamets(word, accentless=False, debug=False, count=False):\n",
    "    dout = []\n",
    "    result = word\n",
    "    if accentless:\n",
    "        result = result.replace('!', '').replace('/', '')\n",
    "    if count: pre = result\n",
    "    result = qamets_qatan1.sub(qamets_qatan_repl, result)\n",
    "    if debug: dout.append(('qamets_qatan1', result))\n",
    "    if count and pre != result: stats['qamets_qatan1'] += 1\n",
    "\n",
    "    if count: pre = result\n",
    "    result = qamets_qatan2.sub(qamets_qatan_repl, result)\n",
    "    if debug: dout.append(('qamets_qatan2', result))\n",
    "    if count and pre != result: stats['qamets_qatan2'] += 1\n",
    "\n",
    "    if count: pre = result\n",
    "    result = qamets_qatan3.sub(qamets_qatan_repl, result)\n",
    "    if debug: dout.append(('qamets_qatan3', result))\n",
    "    if count and pre != result: stats['qamets_qatan3'] += 1\n",
    "\n",
    "    if count: pre = result\n",
    "    result = qamets_qatan4a.sub(qamets_qatan_repl, result)\n",
    "    if debug: dout.append(('qamets_qatan4a', result))\n",
    "    if count and pre != result: stats['qamets_qatan4a'] += 1\n",
    "\n",
    "    if count: pre = result\n",
    "    result = qamets_qatan4b.sub(qamets_qatan_repl, result)\n",
    "    if debug: dout.append(('qamets_qatan4b', result))\n",
    "    if count and pre != result: stats['qamets_qatan4b'] += 1\n",
    "\n",
    "    return (result, dout) if debug else result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schwa and dagesh\n",
    "\n",
    "### Schwa\n",
    "\n",
    "The rules for the schwa that I have found are contradictory.\n",
    "\n",
    "These rules I have seen (e.g.) \n",
    "\n",
    "1. if two consecutive consonants have both a schwa, the second one is mobile;\n",
    "1. a schwa under a consonant with dagesh forte is mobile\n",
    "1. a schwa under the last consonant of a word is quiescens\n",
    "1. a schwa on a consonant that follows a long vowel, is mobile\n",
    "\n",
    "But there are examples where rules 1 and 3 apply at the same time.\n",
    "\n",
    "And the qal 3 sg f forms end with a tav with schwa, often preceded by a consonant with also schwa.\n",
    "In this case the tav has a dagesh, which by the rules for dagesh cannot be a lene. So it must be a forte.\n",
    "So this violates rule 2.\n",
    "\n",
    "We will cut this matter short, and make any final schwa quiescens.\n",
    "\n",
    "As to rule 4, there are cases where the schwa in question is also followed by a final consonant with schwa.\n",
    "In those cases it seems that the schwa in question is silent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mobile schwa\n",
    "mobile_schwa1 = re.compile('''\n",
    "    (                           # here is what goes before the schwa in question\n",
    "        (?:(?:\\A|[ &-]).\\.?)|   # an initial consonant or\n",
    "        (?:.\\.)|                # a consonant with dagesh (which must be forte then) or \n",
    "        (?::.\\.?)|              # another schwa and then a consonant\n",
    "        (?:                     # a long vowel such as the following\n",
    "            (?:\n",
    "                @>?|               # qamets possibly with alef as mater lectionis (the remaining qametses are gadol)\n",
    "                ;j?|               # tsere, possibly followed by yod\n",
    "                ij|                # hireq with yod\n",
    "                o[>w]?|            # holam possibly followed by yod\n",
    "                w\\.                # waw with dagesh\n",
    "            )\n",
    "            {c}                 # and then a consonant\n",
    "        )\n",
    "    )\n",
    "    :\n",
    "    (?![@ae])                   # the schwa may not be composite\n",
    "'''.format(c=cons), re.X)\n",
    "\n",
    "mobile_schwa2 = re.compile(':(?={b}(?:[^.]|[ &-]|\\Z))'.format(b=bgdkpt)) # before BGDKPT letter without dagesh\n",
    "\n",
    "# second last consonant with schwa when last consonsoant also has schwa\n",
    "mobile_schwa3 = re.compile('[%:](?={c}\\.?{a}?[%:](?:[ &]|\\Z))'.format(a=acc, c=cons))\n",
    "\n",
    "# all schwas and the end of the word are quiescens, only if the words are not glued together\n",
    "mobile_schwa4 = re.compile('[%:](?=[ &]|\\Z)')\n",
    "\n",
    "def mobile_schwa1_repl(match):\n",
    "    return match.group(1)+'%'\n",
    "\n",
    "# dagesh\n",
    "dages_forte_lene = re.compile('(?<={v1})(-*)({b})\\.(?=[/!]?{v2})'.format(v1=vowel1, v2=vowel, b=bgdkpt))\n",
    "dages_forte = re.compile('(?<={v1})(-?[h>]*-*)([^h])\\.(?=[/!]?{v2})'.format(v1=vowel1, v2=vowel))\n",
    "dages_lene = re.compile('({b})\\.'.format(b=bgdkpt))\n",
    "\n",
    "def dages_forte_lene_repl(match):\n",
    "    return match.group(1)+(dagesh_lene_dict[match.group(2)] * 2)\n",
    "\n",
    "def dages_lene_repl(match):\n",
    "    return dagesh_lene_dict[match.group(1)]\n",
    "\n",
    "def dages_forte_repl(match):\n",
    "    return match.group(1) + match.group(2) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mater lectionis and final fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silent aleph\n",
    "silent_aleph = re.compile('(?<=[^ &-])>(?!(?:[/!]|{v}))'.format(v=vowel))\n",
    "\n",
    "# final mater lectionis\n",
    "# I assume that heh and alef are only matrices lectionis after a LONG vowel\n",
    "last_ml = re.compile('(?<={v1})[>h]+(?=[ &-]|\\Z)'.format(v1=lvowel1))\n",
    "last_ml_jw = re.compile('jw(?=[ &-]|\\Z)')\n",
    "\n",
    "# mappiq heh\n",
    "mappiq_heh = re.compile('h\\.')\n",
    "\n",
    "fixit_i = re.compile('([{v}])\\.'.format(v=complex_i_vowel))\n",
    "fixit_w = re.compile('([{v}])\\.'.format(v=complex_w_vowel))\n",
    "fixit = re.compile('(.)\\.')\n",
    "\n",
    "split_sep = re.compile('^(.*?)([ .&$\\n-]*)$') # to split the result in the phono part and the interword part\n",
    "\n",
    "def fixit_repl(match):\n",
    "    return match.group(1) * 2\n",
    "\n",
    "def fixit_i_repl(match):\n",
    "    return match.group(1)+'j'\n",
    "\n",
    "def fixit_w_repl(match):\n",
    "    return match.group(1)+'w'\n",
    "\n",
    "# END OF REGULAR EXPRESSIONS AND REPLACEMENT FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qamets corrections\n",
    "\n",
    "For some words we need specific corrections.\n",
    "The rules for qamets qatan are not specific enough.\n",
    "\n",
    "### Correction mechanism\n",
    "\n",
    "We define a function ``apply_corr(wordq, corr)`` that can apply a correction instruction to ``wordq``, which is a word in pre-transliterated form, i.e. a word that has underwent transliteration steps ending with qamets interpretation, including applying special verb cases.\n",
    "\n",
    "The ``corr`` is a comma-separated list of basic instructions, which have the form\n",
    "*number* *letter*. It will interpret the *number*-th qamets as a gadol of qatan, depending on whether *letter* = ``ā`` or ``o``.\n",
    "\n",
    "### Precomputed list of corrections\n",
    "\n",
    "Later on we compile a dictionary ``qamets_corrections`` of pre-computed corrections.\n",
    "This dictionary is keyed by the pre-transliterated form, and valued by the corresponding correction string. Here we initialize this dictionary.\n",
    "\n",
    "The ``phono()`` function that carries out the complete transliteration, looks by default in ``qamets_corrections``, but this can be overridden. These corrections will not be carried out for the special verb cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qamets_corrections = {} # list of translits that must be corrected\n",
    "\n",
    "# apply correction instructions to a word\n",
    "\n",
    "def apply_corr(wordq, corr):\n",
    "    if corr == '': return wordq\n",
    "    corrs = corr.split(',')\n",
    "    indices=[]\n",
    "    for (i, ch) in enumerate(wordq):\n",
    "        if ch == '^' or (ch == '@' and (i == 0 or wordq[i-1] != ':')):\n",
    "            indices.append(i)\n",
    "    resultlist = list(wordq)\n",
    "    for c in corrs:\n",
    "        (pos, kind) = c\n",
    "        pos = int(pos) - 1\n",
    "        repl = '^' if kind == 'o' else '@'\n",
    "        if pos >= len(indices):\n",
    "            msg('Line {}: pos={} out of range {}'.format(ln, pos, indices))\n",
    "            continue\n",
    "        rpos = indices[pos]\n",
    "        resultlist[rpos] = repl\n",
    "    return''.join(resultlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature value normalization\n",
    "\n",
    "We need concise, normalized values for the lexical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "undefs = {'NA', 'unknown', 'n/a', 'absent'}\n",
    "\n",
    "png = dict(\n",
    "    NA='-',\n",
    "    unknown='-',\n",
    "    p1='1',\n",
    "    p2='2',\n",
    "    p3='3',\n",
    "    sg='s',\n",
    "    du='d',\n",
    "    pl='p',\n",
    "    m='m',\n",
    "    f='f',\n",
    "    a='a',\n",
    "    c='c',\n",
    "    e='e',\n",
    ")\n",
    "png['n/a'] = '-'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical info\n",
    "\n",
    "We need a label for lexical information such as part of speech, person, number, gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "declensed = {'subs', 'nmpr', 'adjv', 'prps', 'prde', 'prin'}\n",
    "\n",
    "def get_lex_info(w):\n",
    "    sp = F.sp.v(w)\n",
    "    lex_infos = [sp]\n",
    "    if sp == 'verb':\n",
    "        lex_infos.extend([F.vs.v(w), F.vt.v(w), '{}{}{}'.format(png[F.ps.v(w)], png[F.nu.v(w)], png[F.gn.v(w)])])\n",
    "    elif sp in declensed:\n",
    "        lex_infos.append('{}{}'.format(png[F.nu.v(w)], png[F.gn.v(w)]))\n",
    "    lex_info = ' '.join(lex_infos)\n",
    "    if sp == 'verb' or sp in declensed:\n",
    "        prs = F.g_prs.v(w)\n",
    "        if prs not in undefs:\n",
    "            lex_info += ',{}'.format(prs.lower())\n",
    "    return lex_info\n",
    "\n",
    "def get_decl(lex_info):\n",
    "    if lex_info == None: lex_info = ''\n",
    "    parts = lex_info.split(',')\n",
    "    return lex_info if len(parts) == 1 else parts[0]\n",
    "\n",
    "def get_prs(lex_info):\n",
    "    if lex_info == None: lex_info = ''\n",
    "    parts = lex_info.split(',')\n",
    "    return '' if len(parts) == 1 else parts[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The phono function\n",
    "\n",
    "The definition of the function that generates the phonological transliteration.\n",
    "It is a function with a big definition, so we have broken it in parts.\n",
    "\n",
    "## Phono parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interesting_stats = [\n",
    "    'total',\n",
    "    'qamets_verb_suppress_qatan',\n",
    "    'qamets_prs_suppress_qatan',\n",
    "    'qamets_qatan_corrections',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if suppress_in_verb, phono will suppress qatan interpretation in certain verb paradigmatic forms\n",
    "# if suppress_in_prs, phono will suppress qatan interpreation in pronominal suffixes\n",
    "# if correct is 1, phono will apply individual corrections\n",
    "# if correct is 0, phono will not apply individual corrections\n",
    "# if correct is -1, phono will stop just before applying the qamets qatan corrections and return\n",
    "# the intermediate result\n",
    "\n",
    "def phono_qamets(\n",
    "        ws, result, lex_info,\n",
    "        debug, count, dout,    \n",
    "        suppress_in_verb, suppress_in_prs,\n",
    "        correct, corrections, \n",
    "    ):\n",
    "# qamets qatan\n",
    "\n",
    "# check whether we are in a verb paradigm that requires suppressing qamets => qatan\n",
    "    if count: pre = result\n",
    "    suppr = True\n",
    "    decl = get_decl(lex_info)\n",
    "\n",
    "    if suppress_in_verb:\n",
    "        suppr = False\n",
    "        if decl == '':\n",
    "            if debug: dout.append(('qamets qatan', 'no special verb form invoked'))\n",
    "        elif decl not in qamets_qatan_verb_x:\n",
    "            if debug: dout.append(('qamets qatan', 'no special verb form: {}'.format(decl)))\n",
    "        elif '@' not in result:\n",
    "            if debug: dout.append(('qamets qatan', 'special verb form: no qamets present'))\n",
    "        elif '!' in result:\n",
    "            if debug: dout.append(('qamets qatan', 'special verb form: primary accent present'))\n",
    "            suppr = True\n",
    "        else:\n",
    "            suppr = True\n",
    "            if count: stats['qamets_verb_suppress_qatan'] += 1\n",
    "    else:\n",
    "        if debug: dout.append(('qamets qatan', 'suppression for verb forms is switched off'))\n",
    "        suppr = False\n",
    "    \n",
    "    if suppr:\n",
    "        if debug: dout.append(('qamets qatan', 'special verb form: qatan suppressed for {}'.format(decl)))\n",
    "    else:\n",
    "        if debug:\n",
    "            (result, this_dout) = doplainqamets(result, debug=True, count=count)\n",
    "            dout.extend(this_dout)\n",
    "        else: result = doplainqamets(result, count=count)\n",
    "\n",
    "# check whether we have a pronominal suffix that requires suppressing qamets => qatan\n",
    "\n",
    "    if count: pre = result\n",
    "    suppr = True\n",
    "    prs = get_prs(lex_info)\n",
    "    if suppress_in_prs:\n",
    "        suppr = False\n",
    "        if prs == '':\n",
    "            if debug: dout.append(('qamets qatan', 'no pron suffix indicated'))\n",
    "        elif '@' not in prs:\n",
    "            if debug: dout.append(('qamets qatan', 'pronominal suffix: no qamets present'))\n",
    "        elif not qamets_qatan_prs.search(result):\n",
    "            if debug: dout.append(('qamets qatan', 'pron suffix {}: no qamets qatan present'.format(prs)))\n",
    "        else:\n",
    "            suppr = True\n",
    "            if count: stats['qamets_prs_suppress_qatan'] += 1\n",
    "    else:\n",
    "        if debug: dout.append(('qamets qatan', 'suppression for pron suffix is switched off'))\n",
    "        suppr = False\n",
    "    \n",
    "    if suppr:\n",
    "        result = qamets_qatan_prs.sub('@', result)\n",
    "        if debug:\n",
    "            dout.append(('qamets qatan', 'pron suffix {}: qatan suppressed'.format(prs)))\n",
    "            dout.append(('qamets qatan prs', result))\n",
    "\n",
    "# now change gadol in qatan in front of other qatan\n",
    "    if count: pre = result\n",
    "    result = qamets_qatan5.sub(qamets_qatan_repl, result)\n",
    "    if debug: dout.append(('qamets_qatan5', result))\n",
    "    if count and pre != result: stats['qamets_qatan5'] += 1\n",
    "\n",
    "# handle desired corrections\n",
    "    if count: pre = result\n",
    "    if correct == -1: return (result, True)\n",
    "    if correct == 1 and decl not in qamets_qatan_verb_x:\n",
    "        if corrections == None: corrections = qamets_corrections\n",
    "        parts = result.split('-')\n",
    "        hotpart = parts[-1]\n",
    "        wordq = phono(ws[-1], correct=-1, punct=False)\n",
    "        if wordq in corrections:\n",
    "            hotpartn = apply_corr(hotpart, corrections[wordq])\n",
    "            if debug: dout.append((\n",
    "                'qamets qatan',\n",
    "                'correction: {} => {}'.format(hotpart, hotpartn)\n",
    "            ))\n",
    "            parts[-1] = hotpartn\n",
    "            result = '-'.join(parts)\n",
    "    if debug: dout.append(('qamets_qatan_corr', result))\n",
    "    if count and pre != result: stats['qamets_qatan_corrections'] += 1\n",
    "\n",
    "    return (result, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phono_patterns(result, debug, count, dout):\n",
    "    \n",
    "# mobile schwa\n",
    "    if count: pre = result\n",
    "    result = mobile_schwa1.sub(mobile_schwa1_repl, result)\n",
    "    if debug: dout.append(('mobile_schwa1', result))\n",
    "    if count and pre != result: stats['mobile_schwa1'] += 1\n",
    "\n",
    "    if count: pre = result\n",
    "    result = mobile_schwa2.sub('%', result)\n",
    "    if debug: dout.append(('mobile_schwa2', result))\n",
    "    if count and pre != result: stats['mobile_schwa2'] += 1\n",
    "\n",
    "    if count: pre = result\n",
    "    result = mobile_schwa3.sub('', result)\n",
    "    if debug: dout.append(('mobile_schwa3', result))\n",
    "    if count and pre != result: stats['mobile_schwa3'] += 1\n",
    "\n",
    "    if count: pre = result\n",
    "    result = mobile_schwa4.sub('', result)\n",
    "    if debug: dout.append(('mobile_schwa4', result))\n",
    "    if count and pre != result: stats['mobile_schwa4'] += 1\n",
    "\n",
    "# dagesh\n",
    "    if count: pre = result\n",
    "    result = dages_forte_lene.sub(dages_forte_lene_repl, result)    \n",
    "    if debug: dout.append(('dagesh_forte_lene', result))\n",
    "    if count and pre != result: stats['dagesh_forte_lene'] += 1\n",
    "\n",
    "    if count: pre = result\n",
    "    result = result.replace('ij.', 'Ijj')\n",
    "    result = dages_forte.sub(dages_forte_repl, result)\n",
    "    if debug: dout.append(('dagesh_forte', result))\n",
    "    if count and pre != result: stats['dagesh_forte'] += 1\n",
    "\n",
    "    if count: pre = result\n",
    "    result = dages_lene.sub(dages_lene_repl, result)\n",
    "    if debug: dout.append(('dagesh_lene', result))\n",
    "    if count and pre != result: stats['dagesh_lene'] += 1\n",
    "\n",
    "# silent aleph (but not in tetra)\n",
    "    if count: pre = result\n",
    "    if '[' not in result:\n",
    "        result = silent_aleph.sub('', result)    \n",
    "    if debug: dout.append(('silent_aleph', result))\n",
    "    if count and pre != result: stats['silent_aleph'] += 1\n",
    "\n",
    "# final mater lectionis (but not in tetra)\n",
    "    if count: pre = result\n",
    "    if '[' not in result:\n",
    "        result = last_ml_jw.sub('ʸw', result)\n",
    "        result = last_ml.sub('', result)    \n",
    "    if debug: dout.append(('last_ml', result))\n",
    "    if count and pre != result: stats['last_ml'] += 1\n",
    "\n",
    "# mappiq heh\n",
    "    if count: pre = result\n",
    "    result = mappiq_heh.sub('h', result)\n",
    "    if debug: dout.append(('mappiq_heh', result))\n",
    "    if count and pre != result: stats['mappiq_heh'] += 1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phono_symbols(ws, result, debug, count, dout):\n",
    "\n",
    "# split the result in parts corresponding with the word nodes of the original\n",
    "    resultparts = result.split('-')\n",
    "    results = []\n",
    "    for (i, w) in enumerate(ws):\n",
    "        resultp = resultparts[i]\n",
    "        result = resultp\n",
    "        # masora\n",
    "        if w in qeres: result = '*'+result\n",
    "\n",
    "        for (sym, repl) in sound_dict.items():\n",
    "            result = result.replace(sym, repl)\n",
    "        if debug: dout.append(('symbols', result))\n",
    "\n",
    "        # fix left over dagesh and mappiq\n",
    "        if count: pre = result\n",
    "        result = fixit_i.sub(fixit_i_repl, result)\n",
    "        if debug: dout.append(('fixit_i', result))\n",
    "        if count and pre != result: stats['fixit_i'] += 1\n",
    "\n",
    "        if count: pre = result\n",
    "        result = fixit_w.sub(fixit_w_repl, result)\n",
    "        if debug: dout.append(('fixit_w', result))\n",
    "        if count and pre != result: stats['fixit_w'] += 1\n",
    "\n",
    "        if count: pre = result\n",
    "        result = fixit.sub(fixit_repl, result)\n",
    "        if count and pre != result: stats['fixit'] += 1\n",
    "        if debug: dout.append(('fixit', result))\n",
    "\n",
    "        if count: pre = result\n",
    "        for (sym, repl) in sound_dict2.items():\n",
    "            result = result.replace(sym, repl)\n",
    "        if debug: dout.append(('punct', result))\n",
    "        if count and pre != result: stats['punct'] += 1\n",
    "\n",
    "    # zero width word boundary\n",
    "        if count: pre = result\n",
    "        result = multiple_space.sub(' ', result)\n",
    "        result = result.replace('[ ', '[').replace(' ]', ']') #tetra\n",
    "        if debug: dout.append(('cleanup', result))\n",
    "        if count and pre != result: stats['cleanup'] += 1\n",
    "        results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phono whole\n",
    "Here the rule fabrics are woven together, exceptions invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phono(\n",
    "        ws, \n",
    "        suppress_in_verb=True, suppress_in_prs=True,\n",
    "        correct=1, corrections=None, \n",
    "        inparts=False,\n",
    "        debug=False,\n",
    "        count=False,\n",
    "        punct=True,\n",
    "    ):\n",
    "    if type(ws) is int: ws = [ws]\n",
    "    if count: stats['total'] += 1\n",
    "    dout = []\n",
    "# collect information\n",
    "    orig = ''.join(get_orig(w, punct=True) for w in ws)\n",
    "    lex_info = get_lex_info(ws[-1])\n",
    "# strip punctuation at the end, if needed\n",
    "    if not punct: orig = punctuation.sub('', orig)\n",
    "# account for ketiv-qere if in debug mode\n",
    "    if debug:\n",
    "        for w in ws:\n",
    "            if w in qeres:\n",
    "                dout.append(('ketiv-qere', '{} => {}'.format(F.g_word.v(w), qeres[w])))\n",
    "# accents\n",
    "    if debug: (result, dout) = doaccents(orig, debug=True, count=count)\n",
    "    else: result = doaccents(orig, count=count)\n",
    "# qamets        \n",
    "    (result, deliver) = phono_qamets(\n",
    "        ws, result, lex_info,\n",
    "        debug, count, dout,    \n",
    "        suppress_in_verb, suppress_in_prs,\n",
    "        correct, corrections, \n",
    "    )\n",
    "    if deliver: return (result, dout) if debug else result\n",
    "# patterns\n",
    "    result = phono_patterns(result, debug, count, dout)\n",
    "# symbols\n",
    "    results = phono_symbols(ws, result, debug, count, dout)\n",
    "    result = ''.join(results) if not inparts else results\n",
    "# deliver\n",
    "    return (result, dout) if debug else result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skeleton analysis\n",
    "\n",
    "We have to do more work for the qamets. Sometimes a word form on its own is not enough to determine whether a qamets is gadol or qatan. In those cases, we analyse all occurrences of the same lexeme, and for each syllable position we measure whether an A-like vowel of an O-like vowel tends to occur in that syllable.\n",
    "\n",
    "In order to do that, we need to compute a *vowel skeleton* for each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stripping paradigmatic material\n",
    "\n",
    "A word may have extra syllables, due to inflections, such as plurals, feminine forms, or suffixes. Let us call this the *paradigmatic material* of a word. \n",
    "\n",
    "Now, we strip from the initial vowel skeleton a number of trailing vowels that corresponds\n",
    "to the number of consonants found in the paradigmatic material.\n",
    "This is rather crude, but it will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need the number of letters in a defined value of a morpho feature\n",
    "def len_suffix(v):\n",
    "    if v == None: return 0\n",
    "    if v in undefs: return 0\n",
    "    return len(v.replace('=', '').replace('W', '').replace('J', ''))\n",
    "\n",
    "# we need a function that return 1 for plural/dual subs/adj and for fem adj\n",
    "def len_ending(sp, n, g):\n",
    "    if sp == 'subs': return 1 if n in {'pl', 'du'} else 0\n",
    "    if sp == 'adjv': return 1 if n in {'pl', 'du'} or g in 'f' else 0 \n",
    "    return 0\n",
    "\n",
    "# return the number of consonants in the suffixes\n",
    "def len_morpho(w):\n",
    "    return max((\n",
    "        len_suffix(F.prs.v(w)) + len_suffix(F.uvf.v(w)), \n",
    "        len_ending(F.sp.v(w), F.nu.v(w), F.gn.v(w)),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skeleton patterns\n",
    "\n",
    "Next, we reduce the vowel skeleton to a skeleton pattern. We are not interested in all vowels, only in whether the vowel is a qamets (gadol or qatan), A-like, O-like, or other (which we dub E-like)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the qamets gadol/qatan skeleton\n",
    "qamets_qatan_skel = re.compile('([^@^])')\n",
    "\n",
    "# the vowel skeleton where the qamets gadol/qatan are preserved as @ and ^\n",
    "# another o-like vowel becomes O (holam, qamets chatuf) (no waws nor yods)\n",
    "# another a-like vowel becomes A (patah, patah chatuf) (no alefs)\n",
    "silent_alef_start = re.compile('([ &-]|\\A)>([!/]?(?:[^!/.:;@^aeiou]|\\Z))')\n",
    "\n",
    "def silent_alef_start_repl(match):\n",
    "    return match.group(1)+'E'+match.group(2)\n",
    "\n",
    "qamets_qatan_fullskel = re.compile('''\n",
    "    (\n",
    "        E                                         # replacement of silent initial alef without vowels\n",
    "    |   (?::[@ae]?)                                # a (composite) schwa\n",
    "    |   (?:[;i]j) | (?:ow) | (?:w.)               # a composite vowel   \n",
    "    |   [@a;eiou^]                                # a vowel point\n",
    "    |   .                                         # anything else\n",
    "    )\n",
    "'''.format(c=cons), re.X)\n",
    "\n",
    "def qamets_qatan_fullskel_repl(match):\n",
    "    found = match.group(1)\n",
    "    if found == 'E': return 'E'\n",
    "    if found == '@': return gadol\n",
    "    if found == '^': return qatan\n",
    "    if found in a_like: return 'A'\n",
    "    if found in o_like: return 'O'\n",
    "    if found in e_like: return 'E'\n",
    "    return ''\n",
    "\n",
    "def get_full_skel(w, debug=False):\n",
    "    wordq = phono(w, correct=-1, punct=False)\n",
    "    wordqr = silent_alef_start.sub(silent_alef_start_repl, wordq)\n",
    "    fullskel = qamets_qatan_fullskel.sub(qamets_qatan_fullskel_repl, wordqr)\n",
    "    ending_length = len_morpho(w)\n",
    "    relevant_part = len(fullskel) - ending_length\n",
    "    if debug: print('{}: {} => {} => {} : {} minus {} = {}'.format(\n",
    "        w, orig, wordq, wordqr, fullskel, ending_length, fullskel[0:relevant_part],\n",
    "    ))\n",
    "\n",
    "    return fullskel[0:relevant_part]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qamets gadol qatan: sophisticated\n",
    "\n",
    "A lot of work is needed to get the qamets gadol-qatan right.\n",
    "This involves looking at accents, verb paradigms and special cases among the non-verbs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qamets gadol qatan: non-verbs\n",
    "\n",
    "Sometimes a qamets is gadol or qatan for lexical reasons, i.e. it can not be derived by rules based on the word occurrence itself, but other occurrences have to be invoked.\n",
    "\n",
    "### All candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 1m 20s Looking for non-verb qamets\n",
      " 1m 23s 4062 lexemes and 13450 unique occurrences\n"
     ]
    }
   ],
   "source": [
    "# find lexemes which have an occurrence with a qamets (except verbs)\n",
    "msg(\"Looking for non-verb qamets\")\n",
    "qq_words = set()\n",
    "qq_lex = collections.defaultdict(lambda: [])\n",
    "\n",
    "for w in F.otype.s('word'):\n",
    "    ln = F.language.v(w)\n",
    "    if ln != 'Hebrew': continue\n",
    "    sp = F.sp.v(w)\n",
    "    if sp == 'verb': continue\n",
    "    orig = get_orig(w, punct=False, tetra=False)\n",
    "    if '@' not in orig: continue   # no qamets in word\n",
    "    word = doaccents(orig)\n",
    "    lex = F.lex.v(w)\n",
    "    if word in qq_words: continue\n",
    "    qq_words.add(word)\n",
    "    qq_lex[lex].append(w)\n",
    "msg('{} lexemes and {} unique occurrences'.format(len(qq_lex), len(qq_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering interesting candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 1m 27s Filtering lexemes with varied occurrences\n",
      " 1m 27s 161 interesting lexemes with 1703 unique occurrences\n"
     ]
    }
   ],
   "source": [
    "msg('Filtering lexemes with varied occurrences')\n",
    "qq_varied = collections.defaultdict(lambda: [])\n",
    "nocc = 0\n",
    "for lex in qq_lex:\n",
    "    ws = qq_lex[lex]\n",
    "    if len(ws) == 1: continue\n",
    "    occs = []\n",
    "    skel_set = set()\n",
    "    has_qatan = False\n",
    "    has_gadol = False\n",
    "    for w in ws:\n",
    "        wordq = phono(w, correct=-1, punct=False)\n",
    "        skel = qamets_qatan_skel.sub('', wordq.replace(':@','')).replace('@',gadol).replace('^',qatan)\n",
    "        if gadol in skel: has_gadol = True\n",
    "        if qatan in skel: has_qatan = True\n",
    "        skel_set.add(skel)\n",
    "        occs.append((skel, w))\n",
    "    if len(skel_set) > 1 and has_qatan and has_gadol:\n",
    "        for (skel, w) in occs:\n",
    "            fullskel = get_full_skel(w)\n",
    "            qq_varied[lex].append((skel, fullskel, w))\n",
    "            nocc += 1\n",
    "msg('{} interesting lexemes with {} unique occurrences'.format(len(qq_varied), nocc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guess the qamets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "qamets_qatan_xc = dict(\n",
    "    (x[0], x[1]) for x in (y.split(' => ') for y in qamets_qatan_x.strip().split('\\n'))\n",
    ")\n",
    "qamets_qatan_xcompiled = collections.defaultdict(lambda: {})\n",
    "for (lex, corrstr) in qamets_qatan_xc.items():\n",
    "    corrs = corrstr.split(',')\n",
    "    for corr in corrs:\n",
    "        (pos, ins) = corr\n",
    "        pos = int(pos) - 1\n",
    "        qamets_qatan_xcompiled[lex][pos] = ins\n",
    "\n",
    "def compile_occs(lex, occs):\n",
    "    vowel_counts = collections.defaultdict(lambda: collections.Counter())\n",
    "    for (skel, fullskel, w) in occs:\n",
    "        for (i, c) in enumerate(fullskel):\n",
    "            vowel_counts[i][c] += 1\n",
    "    occs_compiled = {}\n",
    "    for i in sorted(vowel_counts):\n",
    "        vowel_count = vowel_counts[i]\n",
    "        a_ish = vowel_count.get(gadol, 0) + vowel_count.get('A', 0)\n",
    "        o_ish = vowel_count.get(qatan, 0) + vowel_count.get('O', 0)\n",
    "        if a_ish != o_ish: occs_compiled[i] = gadol if a_ish > o_ish else qatan\n",
    "    if lex in qamets_qatan_xcompiled:\n",
    "        override = qamets_qatan_xcompiled[lex]\n",
    "        for i in override:\n",
    "            ins = override[i]\n",
    "            old_ins = occs_compiled.get(i, '')\n",
    "            new_ins = gadol if ins == 'A' else qatan\n",
    "            if old_ins == new_ins:\n",
    "                print('{}: No override needed for syllable {} which is {}'.format(\n",
    "                    lex, i+1, old_ins,\n",
    "                ))\n",
    "            else:\n",
    "                print('{}: Override for syllable {}: {} becomes {}'.format(\n",
    "                    lex, i+1, old_ins, new_ins,\n",
    "                ))\n",
    "                occs_compiled[i] = new_ins\n",
    "    return occs_compiled\n",
    "\n",
    "def guess_qq(occ, occs_compiled, debug=False):\n",
    "    (skel, fullskel, w) = occ\n",
    "    guess = ''\n",
    "    for (i, c) in enumerate(fullskel):\n",
    "        guess += occs_compiled.get(i, c) if c == gadol or c == qatan else c\n",
    "    if debug: print('{}'.format(w))\n",
    "    return guess\n",
    "\n",
    "def get_corr(fullskel, guess, debug=False):\n",
    "    n = 0\n",
    "    corr = []\n",
    "    for (i, fc) in enumerate(fullskel):\n",
    "        if fc != qatan and fc != gadol: continue\n",
    "        n += 1\n",
    "        gc = guess[i]\n",
    "        if fc == gc: continue\n",
    "        corr.append('{}{}'.format(n, gc))\n",
    "    if debug: print('{} guess {} corr {}'.format(fullskel, guess, corr))\n",
    "    return ','.join(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carrying out the guess work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 1m 31s Guessing between gadol and qatan\n",
      " 1m 31s 107 lexemes with modified occurrences (224)\n",
      " 1m 31s 0 patterns with conflicts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BJT/: Override for syllable 1: o becomes ā\n",
      "JRB<M/: No override needed for syllable 1 which is ā\n",
      "JHWNTN/: Override for syllable 2:  becomes ā\n",
      "JM/: Override for syllable 1: ā becomes o\n",
      "JWMM: Override for syllable 2:  becomes ā\n"
     ]
    }
   ],
   "source": [
    "msg('Guessing between gadol and qatan')\n",
    "qamets_corrections = {}\n",
    "qq_varied_remaining = set()\n",
    "ndiff_occs = 0\n",
    "ndiff_lexs = 0\n",
    "nconflicts = 0\n",
    "for lex in qq_varied:\n",
    "    debug = False\n",
    "    occs = qq_varied[lex]\n",
    "    occs_compiled = compile_occs(lex, occs)\n",
    "    this_ndiff_occs = 0\n",
    "    for occ in occs:\n",
    "        (skel, fullskel, w) = occ\n",
    "        guess = guess_qq(occ, occs_compiled, debug=debug)\n",
    "        corr = get_corr(fullskel, guess, debug=debug)\n",
    "        if corr:\n",
    "            this_ndiff_occs += 1\n",
    "            wordq = phono(w, correct=-1, punct=False)\n",
    "            if wordq in qamets_corrections:\n",
    "                old_corr = qamets_corrections[wordq]\n",
    "                if old_corr != corr:\n",
    "                    print('Conflicting corrections for {} {} {} ({} => {}): first {} and then {}'.format(\n",
    "                        lex, wordq, skel, fullskel, guess, old_corr, corr,\n",
    "                    ))\n",
    "                    nconflicts += 1\n",
    "            qamets_corrections[wordq] = corr\n",
    "\n",
    "    if this_ndiff_occs:\n",
    "        ndiff_lexs += 1\n",
    "        ndiff_occs += this_ndiff_occs\n",
    "        qq_varied_remaining.add(lex)\n",
    "msg('{} lexemes with modified occurrences ({})'.format(ndiff_lexs, ndiff_occs))\n",
    "msg('{} patterns with conflicts'.format(nconflicts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "The function below reads a text file with tests.\n",
    "\n",
    "A test is a tab separated line with as fields:\n",
    "\n",
    "    passage etcbc-original phono_transcription expected_result bol_reference comments\n",
    "    \n",
    "The testing routine executes all tests, checks the results, produces onscreen output, debug output in file, and pretty output in a html file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions\n",
    "\n",
    "### Composing tests\n",
    "\n",
    "Given an occurrence in etcbc translit in a passage, or a node number, we want to easily compile a test out of it.\n",
    "Say we are looking for ``orig``.\n",
    "\n",
    "The match need not be perfect. \n",
    "We want to find the node w, which carries a translit that occurs at the end of ``orig``.\n",
    "If there are multiple, we want the longest.\n",
    "If there are multiple longest ones, we want the first that occurs in the passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hebrew(orig):\n",
    "    origm = Transcription.suffix_and_finales(orig)\n",
    "    return Transcription.to_hebrew(origm[0]+origm[1]).replace('-','')\n",
    "\n",
    "def get_passage(w):\n",
    "    vn = w if F.otype.v(w) == 'verse' else L.u('verse', w)\n",
    "    return '{} {}:{}'.format(\n",
    "        F.book.v(L.u('book', w)),\n",
    "        F.chapter.v(L.u('chapter', w)),\n",
    "        F.verse.v(vn),\n",
    "    )\n",
    "\n",
    "def maketest(ws=None, orig=None, passage=None, expected=None, comment=None):\n",
    "    if comment == None: comment = 'isolated case'\n",
    "    if ws == None:\n",
    "        if passage != None and orig != None: ws = find_w(passage, orig)\n",
    "    if ws == None:\n",
    "        print('Cannot make test: {}: {} not found'.format(passage, orig))\n",
    "        return None\n",
    "    else:\n",
    "        if type(ws) is int: ws = [ws]\n",
    "        passage = get_passage(ws[-1])            \n",
    "        if expected == None: expected = phono(ws, punct=False)\n",
    "        test = (ws, expected.rstrip(' '), comment)\n",
    "    return test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting test results\n",
    "\n",
    "Here are some HTML/CSS definitions for pretty printing test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_esc(txt):\n",
    "    return txt.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')\n",
    "\n",
    "def test_html_head(title, stats, mystats): \n",
    "    return '''<html>\n",
    "<head>\n",
    "    <meta http-equiv=\"Content-Type\"\n",
    "          content=\"text/html; charset=UTF-8\" />\n",
    "    <title>'''+title+'''</title>\n",
    "    <style type=\"text/css\">\n",
    "        .h {\n",
    "            font-family: Ezra SIL, SBL Hebrew, Verdana, sans-serif; \n",
    "            font-size: x-large;\n",
    "            text-align: right;\n",
    "        }\n",
    "        .t {\n",
    "            font-family: Menlo, Courier New, Courier, monospace;\n",
    "            font-size: small;\n",
    "            color: #0000cc;        \n",
    "        }\n",
    "        .tl {\n",
    "            font-family: Menlo, Courier New, Courier, monospace;\n",
    "            font-size: medium;\n",
    "            font-weight: bold;\n",
    "            color: #000000;        \n",
    "        }\n",
    "        .p {\n",
    "            font-family: Verdana, Arial, sans-serif;\n",
    "            font-size: medium;\n",
    "        }\n",
    "        .l {\n",
    "            font-family: Verdana, Arial, sans-serif;\n",
    "            font-size: small;\n",
    "            color: #440088;\n",
    "        }\n",
    "        .v {\n",
    "            font-family: Verdana, Arial, sans-serif; \n",
    "            font-size: x-small;\n",
    "            color: #666666;\n",
    "        }\n",
    "        .c {\n",
    "            font-family: Ezra SIL, SBL Hebrew, Verdana, sans-serif;\n",
    "            font-size: small;\n",
    "            background-color: #ffffdd;\n",
    "            width: 20%;\n",
    "        }\n",
    "        .cor {\n",
    "            font-family: Menlo, Courier New, Courier, monospace;\n",
    "            font-weight: bold\n",
    "            font-size: medium;\n",
    "        }\n",
    "        .exact {\n",
    "            background-color: #88ffff;\n",
    "        }\n",
    "        .good {\n",
    "            background-color: #88ff88;\n",
    "        }\n",
    "        .error {\n",
    "            background-color: #ff8888;\n",
    "        }\n",
    "        .norm {\n",
    "            background-color: #8888ff;\n",
    "        }\n",
    "        .ca {\n",
    "            background-color: #88ffff;\n",
    "        }\n",
    "        .cr {\n",
    "            background-color: #ffff33;\n",
    "        }\n",
    "    </style>\n",
    "</head><body>\n",
    "'''+(('<p>'+stats+'</p>') if stats else '')+(('<p>'+mystats+'</p>') if mystats else '')+'''\n",
    "<table>\n",
    "'''\n",
    "\n",
    "test_html_tail = '''</table>\n",
    "</body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run tests\n",
    "\n",
    "This is the function that runs a sequence of tests.\n",
    "If the second argument is a string, it reads a tab separated file with tests from a file with that name.\n",
    "Otherwise it should be a list of tests, a test being a list or tuple consisting of:\n",
    "\n",
    "    source, orig, lex_info, expected, comment\n",
    "    \n",
    "where ``source`` is either a string ``passage`` or a number ``w``.\n",
    "If it is a ``w``, it is the node corresponding to the word, and it is used to get the ``passage, orig, lex_info`` which are allowed to be empty.\n",
    "If it is a ``passage``, the node will be looked up on the basis of it plus ``orig``.\n",
    "If the node is found, it will be used to get the ``lex_info``, if not, the given ``lex_info`` will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def vfname(inpath):\n",
    "    (indir, infile) = os.path.split(inpath)\n",
    "    (inbase, inext) = os.path.splitext(infile)\n",
    "    return os.path.join(indir, inbase+version+inext)\n",
    "    \n",
    "def runtests(title, testsource, outfilename, htmlfilename, order=True, screen=False):\n",
    "    skipped = 0\n",
    "    if type(testsource) is list:\n",
    "        tests = testsource\n",
    "    else:\n",
    "        tests = []\n",
    "        test_in_file = open(testsource)\n",
    "        for tline in test_in_file:\n",
    "            (passage, orig, expected, comment) = (tline.rstrip('\\n').split('\\t'))\n",
    "            this_test = maketest(orig=orig, passage=passage, expected=expected, comment=comment)\n",
    "            if this_test != None:\n",
    "                tests.append(this_test)\n",
    "            else:\n",
    "                skipped += 1\n",
    "        test_in_file.close()\n",
    "\n",
    "    lines = []\n",
    "    htmllines = []\n",
    "    longlines = []\n",
    "    nexact = 0\n",
    "    ngood = 0\n",
    "    ntests = len(tests)\n",
    "    test_sequence = sorted(tests, key=lambda x: (x[1], x[2], x[0])) if order else tests\n",
    "\n",
    "    for (i, (wset, expected, comment)) in enumerate(test_sequence):\n",
    "        passage = get_passage(wset[-1])\n",
    "        wss = partition_w(wset)\n",
    "        orig = ''.join(get_orig(w, punct=True, set_pet=True, tetra=False) for w in wset)\n",
    "        wordph = ''\n",
    "        lex_info = ''\n",
    "        dout = []\n",
    "        for (j, ws) in enumerate(wss):\n",
    "            this_lex_info = get_lex_info(ws[-1])\n",
    "            (this_wordph, this_dout) = phono(ws, punct=not (j == len(wss) - 1), debug=True)\n",
    "            wordph += this_wordph\n",
    "            lex_info += this_lex_info\n",
    "            dout.extend(this_dout)\n",
    "        wordph = wordph.rstrip(' ')\n",
    "        if wordph == expected:\n",
    "            isgood = '='\n",
    "            nexact += 1\n",
    "        elif wordph.replace('ˌ', '').replace('ˈ', '').replace('-', '') == \\\n",
    "             expected.replace('ˌ', '').replace('ˈ', '').replace('-', ''):\n",
    "            isgood = '~'\n",
    "            ngood += 1\n",
    "        else:\n",
    "            isgood = '#'\n",
    "        line_text = '{:>3} {:<19} {:>6} {:<17} {:<22} {:<20} {} {:<20}'.format(\n",
    "            i+1, passage, ws[-1],\n",
    "            lex_info, orig, wordph,\n",
    "            isgood,\n",
    "            '' if isgood == '=' else expected, \n",
    "        )\n",
    "        lines.append(line_text)\n",
    "        if screen:\n",
    "            if isgood in {'=', '~'}:\n",
    "                print(line_text)\n",
    "        if isgood not in {'=', '~'}:\n",
    "            print(line_text)\n",
    "        longlines.append('{:>3} {:<19} {:>6} {:<17} {:<25} => {:<25} < {} {:<25} # {}\\n{}\\n\\n'.format(\n",
    "            i+1, passage, ws[-1],\n",
    "            lex_info, orig, wordph,\n",
    "            isgood,\n",
    "            '' if isgood == '=' else expected, \n",
    "            comment,\n",
    "            '\\n'.join('{:<7} {:<20} {}'.format('', x[0], x[1]) for x in dout),\n",
    "        ))\n",
    "        htmllines.append(('''\n",
    "    <tr>\n",
    "        <td class=\"{st}\">{i}</td>\n",
    "        <td class=\"v\">{v} {w}</td>\n",
    "        <td class=\"t\">{t}</td>        \n",
    "        <td class=\"h\">{h}</td>\n",
    "        <td class=\"l\">{l}</td>\n",
    "        <td class=\"p {st}\">{p}</td>\n",
    "        <td class=\"p{est}\">{e}</td>\n",
    "        <td class=\"c\">{c}</td>\n",
    "    </tr>\n",
    "    ''').format(\n",
    "        st='exact' if isgood == '=' else 'good' if isgood == '~' else 'error',\n",
    "        i=i+1,\n",
    "        v=passage, w='' if w == None else w,\n",
    "        t=h_esc(orig),\n",
    "        l=lex_info,\n",
    "        h=get_hebrew(orig),\n",
    "        p=wordph,\n",
    "        e='' if isgood == '=' else expected,\n",
    "        est='' if isgood == '=' else ' ca' if isgood == '~' else ' norm',\n",
    "        c=h_esc(comment),\n",
    "    ))\n",
    "\n",
    "    line_text = '\\n'.join(lines)\n",
    "    longline_text = '\\n'.join(longlines)\n",
    "    test_out_file = open(vfname(outfilename), 'w')\n",
    "    test_out_file.write('{}\\n\\n{}\\n'.format(line_text, longline_text))\n",
    "    stats = '{} tests; {} skipped; {} failed; {} passed of which {} exactly.'.format(\n",
    "        ntests + skipped, skipped, ntests-ngood-nexact, ngood + nexact, nexact,\n",
    "    )\n",
    "    msg('ntests={}, skipped={}, ngood={}, nexact={}'.format(ntests, skipped, ngood, nexact))\n",
    "    test_out_file.close()\n",
    "    test_html_file = open(vfname(htmlfilename), 'w')\n",
    "    test_html_headline = '''\n",
    "    <tr>\n",
    "        <th class=\"v\">v</th>\n",
    "        <th class=\"v\">verse</th>\n",
    "        <th class=\"t\">etcbc</th>\n",
    "        <th class=\"h\">hebrew</th>\n",
    "        <th class=\"l\">lexical</th>\n",
    "        <th class=\"p\">phono</th>\n",
    "        <th class=\"p norm\">expected</th>\n",
    "        <th class=\"c\">comment</th>\n",
    "    </tr>\n",
    "    '''\n",
    "    test_html_file.write('{}{}{}{}'.format(\n",
    "        test_html_head(title, stats, ''), test_html_headline, ''.join(htmllines), test_html_tail))\n",
    "    test_html_file.close()\n",
    "    msg(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce showcases\n",
    "\n",
    "This is a variant on ``runtests()``.\n",
    "\n",
    "It produces overviews of the cases where the corpus dependent rules have been applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showcases(title, stats, testsource, order=True):\n",
    "    ctitle = title+' cases'\n",
    "    ttitle = title+' tests'\n",
    "    fctitle = ctitle.replace(' ', '_')\n",
    "    fttitle = ttitle.replace(' ', '_')\n",
    "    test_file_name = my_file(vfname(fttitle+'.txt'))\n",
    "    html_file_name = vfname(fctitle+'.html')\n",
    "\n",
    "    msg(\"Generating HTML in {}\".format(html_file_name))\n",
    "    msg(\"Generating test set {} in {}\".format(title, test_file_name))\n",
    "\n",
    "    htmllines = []\n",
    "    ncorr = 0\n",
    "    test_sequence = sorted(\n",
    "        testsource, key=lambda x: (x[3], x[0], x[1], x[5])\n",
    "    ) if order else testsource\n",
    "    ntests = len(testsource)\n",
    "\n",
    "    test_file = open(test_file_name, 'w')\n",
    "    for (i, (corr, wordph, wordph_c, lex, orig, w, comment)) in enumerate(test_sequence):\n",
    "        passage = get_passage(w)\n",
    "        lex_info = get_lex_info(w)\n",
    "        test_file.write('{}\\t{}\\t{}\\t{}\\n'.format(\n",
    "            passage,\n",
    "            orig,\n",
    "            wordph_c,\n",
    "            comment,\n",
    "        ))\n",
    "        heb = get_hebrew(orig)\n",
    "        if corr:\n",
    "            ncorr += 1\n",
    "        htmllines.append(('''\n",
    "    <tr>\n",
    "        <td class=\"v\">{i}</td>\n",
    "        <td class=\"cor{st}\">{cr}</td>\n",
    "        <td class=\"tl\">{tl}</td>        \n",
    "        <td class=\"v\">{v} {w}</td>\n",
    "        <td class=\"l\">{l}</td>\n",
    "        <td class=\"h\">{h}</td>\n",
    "        <td class=\"p {st}\">{p}</td>\n",
    "        <td class=\"p {st1}\">{pc}</td>\n",
    "        <td class=\"t\">{t}</td>        \n",
    "        <td class=\"c\">{c}</td>\n",
    "    </tr>\n",
    "''').format(\n",
    "        i=i+1,\n",
    "        st=' cr' if corr else '',\n",
    "        st1=' good' if corr else '',\n",
    "        cr=corr,\n",
    "        tl=h_esc(lex),\n",
    "        v=passage, w='' if w == None else w,\n",
    "        l=lex_info,\n",
    "        h=heb,\n",
    "        p=wordph if wordph != wordph_c else '',\n",
    "        pc=wordph_c,\n",
    "        t=h_esc(orig),\n",
    "        c=h_esc(comment),\n",
    "    ))\n",
    "    test_file.close()\n",
    "\n",
    "    mystats = '{} occurrences and {} corrections'.format(\n",
    "        ntests, ncorr,\n",
    "    )\n",
    "    test_html_headline = '''\n",
    "    <tr>\n",
    "        <th class=\"v\">n</th>\n",
    "        <th class=\"cor cr\">correction</th>\n",
    "        <th class=\"tl\">lexeme</th>\n",
    "        <th class=\"v\">verse</th>\n",
    "        <th class=\"l\">lexical</th>\n",
    "        <th class=\"h\">hebrew</th>\n",
    "        <th class=\"p cr\">phono<br/>uncorrected</th>\n",
    "        <th class=\"p good\">phono<br/>corrected</th>\n",
    "        <th class=\"t\">etcbc</th>\n",
    "        <th class=\"c\">comment</th>\n",
    "    </tr>\n",
    "    '''\n",
    "    test_html_file = open(html_file_name, 'w')\n",
    "    test_html_file.write('{}{}{}{}'.format(\n",
    "        test_html_head(ctitle, stats, mystats), test_html_headline, ''.join(htmllines), test_html_tail))\n",
    "    test_html_file.close()\n",
    "    if stats: msg(stats)\n",
    "    if mystats: msg(mystats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the existing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 1m 39s ntests=86, skipped=0, ngood=19, nexact=67\n",
      " 1m 39s 86 tests; 0 skipped; 0 failed; 86 passed of which 67 exactly.\n",
      " 1m 40s ntests=1574, skipped=0, ngood=197, nexact=1377\n",
      " 1m 40s 1574 tests; 0 skipped; 0 failed; 1574 passed of which 1377 exactly.\n",
      " 1m 40s ntests=513, skipped=0, ngood=30, nexact=483\n",
      " 1m 40s 513 tests; 0 skipped; 0 failed; 513 passed of which 483 exactly.\n",
      " 1m 40s ntests=209, skipped=0, ngood=30, nexact=179\n",
      " 1m 40s 209 tests; 0 skipped; 0 failed; 209 passed of which 179 exactly.\n"
     ]
    }
   ],
   "source": [
    "for tname in [\n",
    "    'mixed',\n",
    "    'qamets_nonverb_tests',\n",
    "    'qamets_verb_tests',\n",
    "    'qamets_prs_tests',\n",
    "]:\n",
    "    runtests(\n",
    "        tname, '{}.txt'.format(tname), \n",
    "        '{}_debug.txt'.format(tname), \n",
    "        '{}.html'.format(tname), \n",
    "        screen=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing: Special cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 1m 23s 14 tests; 0 skipped; 0 failed; 14 passed of which 14 exactly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Genesis 9:21          4419 subs sm,+h        >@H:@LO75W00           *ʔohᵒlˈô             =                     \n",
      "  2 Samuel_I 23:4       155387 nmpr sm,          JHW@80H_S              [yhwˈāh]             =                     \n",
      "  3 Genesis 4:1           1684 nmpr sm,          J:HW@75H00             [yᵊhwˈāh]            =                     \n",
      "  4 Genesis 4:1           1684 nmpr sm,          J:HW@75H00             [yᵊhwˈāh]            =                     \n",
      "  5 Genesis 1:27           538 subs sm,          H@95->@D@M03           hˈāʔāḏˌām            =                     \n",
      "  6 Genesis 1:1              5 art               HA-                    hˌa                  =                     \n",
      "  7 Genesis 1:7            107 subs sm,          MI-T.A74XAT            mittˈaḥaṯ            =                     \n",
      "  8 Genesis 1:7            106 prep              MI-                    mˌi                  =                     \n",
      "  9 Genesis 1:7            107 subs sm,          T.A74XAT               tˈaḥaṯ               =                     \n",
      " 10 Genesis 1:1              6 subs pm,          C.@MA73JIM             ššāmˌayim            =                     \n",
      " 11 Genesis 48:9         27477 verb piel impf 1s-,+;m >:AB@R:AK;75M00        ʔᵃvārᵃḵˈēm           =                     \n",
      " 12 Proverbia 10:10     349407 subs sf,          <AY.@92BET             ʕaṣṣˈāveṯ            =                     \n",
      " 13 Genesis 17:11         7494 subs sf,+kem      <@R:LAT:KE92M          ʕorlaṯᵊḵˈem          =                     \n",
      " 14 Joel 1:17           294291 verb qal perf 3p-, <@B:C74W.              ʕāvᵊšˈû              =                     \n"
     ]
    }
   ],
   "source": [
    "special_tests = [\n",
    "    dict(passage='Joel 1:17', orig='<@B:C74W.', comment='qamets gadol or qatan'),\n",
    "    dict(ws=7494, expected=None, comment=\"schwa in front of BGDKPT without dagesh\"),\n",
    "    dict(ws=5, expected=None, comment=\"article in isolation\"),\n",
    "    dict(ws=6, expected=None, comment=\"word after article in isolation\"),\n",
    "    dict(ws=106, expected=None, comment=\"proclitic min\"),\n",
    "    dict(ws=107, expected=None, comment=\"word starting with BGDKPT after proclitic min\"),\n",
    "    dict(passage='Genesis 1:7', orig='MI-T.A74XAT', expected=None, comment=\"proclitic min combined with word starting with BGDKPT\"),\n",
    "    dict(ws=1684, expected=None, comment='Tetra with end of verse'),\n",
    "    dict(passage='Genesis 4:1', orig='J:HW@75H00', expected=None, comment='Tetra with end of verse'),\n",
    "    dict(ws=27477, expected=None, comment=\"pronominal suffix after verb\"),\n",
    "    dict(ws=155387, expected=None, comment=\"peculiar representation of tetragrammaton\"),\n",
    "    dict(passage='Proverbia 10:10', orig='<AY.@92BET', expected=None, comment=\"the qamets should be gadol\"),\n",
    "    dict(passage='Genesis 9:21', orig='*>HLH', expected=None, comment='ketiv qere'),\n",
    "    dict(passage='Genesis 1:27', orig='H@95->@D@M03', expected=None, comment='qamets gadol'),\n",
    "]\n",
    "\n",
    "compiled_tests = []\n",
    "for t in special_tests:\n",
    "    this_test = maketest(**t)\n",
    "    if this_test != None:\n",
    "        compiled_tests.append(this_test)\n",
    "\n",
    "runtests(\n",
    "    'special cases',\n",
    "    compiled_tests, \n",
    "    'special_cases_out.txt', 'special_cases.html',\n",
    "    screen=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making new tests: Qamets gadol qatan: non-verbs\n",
    "\n",
    "We have generated a number of corrections of the qamets interpretation in non verbs. We have applied exceptions to the corrections. Here is the list of representative occurrences where corrections and/or exceptions have been applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 1m 26s 107 lexemes with 1190 occurrences and 224 corrections written\n",
      " 1m 26s Generating HTML in qamets_nonverb_cases4b.html\n",
      " 1m 26s Generating test set qamets nonverb in /Users/dirk/SURFdrive/laf-fabric-output/etcbc4b/phono/qamets_nonverb_tests4b.txt\n",
      " 1m 26s 107 lexemes\n",
      " 1m 26s 1190 occurrences and 224 corrections\n"
     ]
    }
   ],
   "source": [
    "### msg('Showing lexemes with varied occurrences')\n",
    "qqi_filename = 'qamets_qatan_individuals'\n",
    "qqi = outfile('{}.txt'.format(qqi_filename))\n",
    "nvcases = []\n",
    "\n",
    "noccs = 0\n",
    "ncorrs = 0\n",
    "for lex in sorted(qq_varied):\n",
    "    if lex not in qq_varied_remaining: continue\n",
    "    occs = qq_varied[lex]\n",
    "    for (skel, fullskel, w) in sorted(occs, key=lambda x: (x[1], x[2])):\n",
    "        orig = get_orig(w, punct=False, tetra=False)\n",
    "        wordq = phono(w, punct=False, correct=-1)\n",
    "        corr = qamets_corrections.get(wordq, '')\n",
    "        if corr: ncorrs += 1\n",
    "        noccs += 1\n",
    "        wordph = phono(w, punct=False, correct=0)\n",
    "        wordph_c = phono(w, punct=False, correct=1)\n",
    "        comment = 'on the basis of other occurrences' if corr else 'by the rules'\n",
    "        qqi.write('{:<1}\\t{:<5}\\t{:<16}\\t{:<16}\\t{:<10}\\t{:<20}\\t{}\\n'.format(\n",
    "            '*' if corr else '',\n",
    "            corr,\n",
    "            wordph,\n",
    "            wordph_c,\n",
    "            lex,\n",
    "            orig,\n",
    "            w,\n",
    "        ))\n",
    "        nvcases.append((corr, wordph, wordph_c, lex, orig, w, comment))\n",
    "    qqi.write('\\n')\n",
    "qqi.close()\n",
    "msg('{} lexemes with {} occurrences and {} corrections written'.format(\n",
    "    len(qq_varied_remaining), noccs, ncorrs,\n",
    "))\n",
    "showcases(\n",
    "    'qamets nonverb',\n",
    "    '{} lexemes'.format(len(qq_varied_remaining)),\n",
    "    nvcases, \n",
    "    order=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making new tests: Qamets gadol qatan: verbs\n",
    "\n",
    "Usually, accents take care that potential qatans are read as gadols.\n",
    "But sometimes the accents are missing.\n",
    "We have used a list of paradigm labels where such cases might occur, and there we suppress the gamets-as-qatan interpretation.\n",
    "We look at the verb paradigms to fill in the missing information.\n",
    "\n",
    "Here we list the cases where this occurs, and show them.\n",
    "\n",
    "### Look up the cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 1m 28s Finding qamets qatan special verb cases\n",
      " 1m 30s 524 cases\n"
     ]
    }
   ],
   "source": [
    "qq_verb_words = set()\n",
    "qq_verb_specials = []\n",
    "\n",
    "msg('Finding qamets qatan special verb cases')\n",
    "for w in F.otype.s('word'):\n",
    "    ln = F.language.v(w)\n",
    "    if ln != 'Hebrew': continue\n",
    "    sp = F.sp.v(w)\n",
    "    if sp != 'verb': continue\n",
    "    orig = get_orig(w, punct=False, tetra=False)\n",
    "    if '@' not in orig: continue   # no qamets in word\n",
    "    word = doaccents(orig)\n",
    "    wordq = doplainqamets(word, accentless=True)\n",
    "    if '^' not in wordq: continue  # no risk of unwanted qamets qatan\n",
    "#    if '!' in word: continue       # primary accent has been marked\n",
    "\n",
    "    lex_info = get_lex_info(w)\n",
    "    decl = get_decl(lex_info)\n",
    "    if decl in qamets_qatan_verb_x:\n",
    "        if (word, lex_info) in qq_verb_words: continue\n",
    "        qq_verb_words.add((word, lex_info))\n",
    "        qq_verb_specials.append((w, orig, word))\n",
    "msg('{} cases'.format(len(qq_verb_specials)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 1m 32s Showing verb cases\n",
      " 1m 32s Generating HTML in qamets_verb_cases4b.html\n",
      " 1m 32s Generating test set qamets verb in /Users/dirk/SURFdrive/laf-fabric-output/etcbc4b/phono/qamets_verb_tests4b.txt\n",
      " 1m 32s 192 lexemes\n",
      " 1m 32s 524 occurrences and 300 corrections\n"
     ]
    }
   ],
   "source": [
    "msg('Showing verb cases')\n",
    "\n",
    "ncorr = 0\n",
    "ngood = 0\n",
    "vcases = []\n",
    "verb_lexemes = set()\n",
    "for (w, orig, word) in qq_verb_specials:\n",
    "    wordph = phono(w, punct=False)\n",
    "    wordph_ns = phono(w, punct=False, suppress_in_verb=False)\n",
    "    corr = ''\n",
    "    lex = F.lex.v(w)\n",
    "    verb_lexemes.add(lex)\n",
    "    if wordph == wordph_ns: \n",
    "        ngood += 1\n",
    "        corr = ''\n",
    "        comment = \"qamets: no need to suppress qatan\"\n",
    "    else:\n",
    "        ncorr += 1\n",
    "        corr = 'gadol'\n",
    "        comment = 'qamets: gadol maintained because of verb paradigm'\n",
    "    vcases.append((corr, wordph_ns, wordph, lex, orig, w, comment))\n",
    "\n",
    "showcases(\n",
    "    'qamets verb',\n",
    "    '{} lexemes'.format(len(verb_lexemes)),\n",
    "    vcases,\n",
    "    order=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making new tests: Qamets gadol qatan: pronominal suffixes\n",
    "\n",
    "Usually, rules involving closed unaccented syllables trigger the qatan interpretation of a qamets.\n",
    "But in pronominal suffixes a qamets is always gadol.\n",
    "We detect these cases and suppress the gamets-as-qatan interpretation there.\n",
    "\n",
    "### Look up the cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 1m 34s Finding qamets qatan in pronominal suffixes\n",
      " 1m 38s 197 potential cases\n"
     ]
    }
   ],
   "source": [
    "qq_prs_words = set()\n",
    "qq_prs_specials = []\n",
    "\n",
    "msg('Finding qamets qatan in pronominal suffixes')\n",
    "for w in F.otype.s('word'):\n",
    "    ln = F.language.v(w)\n",
    "    if ln != 'Hebrew': continue\n",
    "    lex_info = get_lex_info(w)\n",
    "    prs = get_prs(lex_info)\n",
    "    if prs == '' : continue\n",
    "    orig = get_orig(w, punct=False, tetra=False)\n",
    "    if '@' not in prs: continue   # no qamets in suffix\n",
    "    word = doaccents(orig)\n",
    "    wordq = doplainqamets(word, accentless=False)\n",
    "    if '^' not in wordq: continue  # no risk of unwanted qamets qatan\n",
    "    if (word, lex_info) in qq_prs_words: continue\n",
    "    qq_prs_words.add((word, lex_info))\n",
    "    qq_prs_specials.append((w, orig, word))\n",
    "msg('{} potential cases'.format(len(qq_prs_specials)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 1m 39s Showing prs cases\n",
      " 1m 40s Generating HTML in qamets_prs_cases4b.html\n",
      " 1m 40s Generating test set qamets prs in /Users/dirk/SURFdrive/laf-fabric-output/etcbc4b/phono/qamets_prs_tests4b.txt\n",
      " 1m 40s 131 lexemes\n",
      " 1m 40s 197 occurrences and 60 corrections\n"
     ]
    }
   ],
   "source": [
    "msg('Showing prs cases')\n",
    "\n",
    "ncorr = 0\n",
    "ngood = 0\n",
    "pcases = []\n",
    "prs_lexemes = set()\n",
    "for (w, orig, word) in qq_prs_specials:\n",
    "    lex = F.lex.v(w)\n",
    "    prs_lexemes.add(lex)\n",
    "    wordph = phono(w)\n",
    "    wordph_ns = phono(w, suppress_in_prs=False)\n",
    "    corr = ''\n",
    "    if wordph == wordph_ns: \n",
    "        ngood += 1\n",
    "        corr = ''\n",
    "        comment = \"qamets: no need to suppress qatan\"\n",
    "    else:\n",
    "        ncorr += 1\n",
    "        corr = 'gadol'\n",
    "        comment = 'qamets: gadol maintained in pronominal suffix'\n",
    "    pcases.append((corr, wordph_ns, wordph, lex, orig, w, comment))\n",
    "\n",
    "showcases(\n",
    "    'qamets prs',\n",
    "    '{} lexemes'.format(len(prs_lexemes)),\n",
    "    pcases,\n",
    "    order=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the whole text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 1m 44s Generating complete texts (transcribed and phonetic) ... \n",
      " 1m 47s  1000 verses 13315 62 0 21\n",
      " 1m 49s  2000 verses 27404 123 2 79\n",
      " 1m 52s  3000 verses 40960 174 5 125\n",
      " 1m 54s  4000 verses 54140 242 8 143\n",
      " 1m 56s  5000 verses 67146 307 13 171\n",
      " 1m 59s  6000 verses 82442 393 15 196\n",
      " 2m 02s  7000 verses 97541 456 17 254\n",
      " 2m 05s  8000 verses 113738 528 18 287\n",
      " 2m 08s  9000 verses 129591 572 20 327\n",
      " 2m 11s 10000 verses 146206 623 20 438\n",
      " 2m 13s 11000 verses 159798 748 20 487\n",
      " 2m 15s 12000 verses 174181 890 24 524\n",
      " 2m 18s 13000 verses 190544 1017 28 576\n",
      " 2m 21s 14000 verses 205093 1167 32 622\n",
      " 2m 23s 15000 verses 218599 1289 33 728\n",
      " 2m 25s 16000 verses 227933 1335 39 777\n",
      " 2m 26s 17000 verses 235623 1378 48 827\n",
      " 2m 28s 18000 verses 243246 1395 51 866\n",
      " 2m 29s 19000 verses 250697 1428 59 906\n",
      " 2m 31s 20000 verses 260106 1469 60 960\n",
      " 2m 33s 21000 verses 275071 1532 63 979\n",
      " 2m 35s 22000 verses 286429 1589 65 1007\n",
      " 2m 38s 23000 verses 301285 1644 66 1075\n",
      " 2m 39s 23213 verses done 304783 1649 66 1081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  270174 accents\n",
      "    9090 cleanup\n",
      "   45235 dagesh_forte\n",
      "   21511 dagesh_forte_lene\n",
      "   59609 dagesh_lene\n",
      "   16321 default_accent\n",
      "     968 fixit\n",
      "    2658 furtive_patah\n",
      "   28195 last_ml\n",
      "    2201 mappiq_heh\n",
      "   93899 mobile_schwa1\n",
      "    2255 mobile_schwa2\n",
      "     179 mobile_schwa3\n",
      "    7702 mobile_schwa4\n",
      "   25587 punct\n",
      "   25498 punctuation\n",
      "      66 qamets_prs_suppress_qatan\n",
      "    5257 qamets_qatan1\n",
      "     243 qamets_qatan2\n",
      "    1791 qamets_qatan3\n",
      "      28 qamets_qatan4a\n",
      "     256 qamets_qatan4b\n",
      "     209 qamets_qatan5\n",
      "    1081 qamets_qatan_corrections\n",
      "    1649 qamets_verb_suppress_qatan\n",
      "      12 rafe\n",
      "   21098 silent_aleph\n",
      "  304783 total\n",
      "  304779 trim\n"
     ]
    }
   ],
   "source": [
    "def stats_prog(): return ' '.join(str(stats.get(stat, 0)) for stat in interesting_stats)\n",
    "        \n",
    "msg('Generating complete texts (transcribed and phonetic) ... ')\n",
    "\n",
    "phono_fname = 'phono{}.txt'.format(version)\n",
    "word_dir = '{}/{}'.format(API['data_dir'], 'ph')\n",
    "if not os.path.exists(word_dir): os.makedirs(word_dir)\n",
    "\n",
    "word_fname = '{}/phono.{}{}'.format(word_dir, source, version)\n",
    "phono_file = outfile(phono_fname)\n",
    "word_file = open(word_fname, 'w')\n",
    "\n",
    "orig_file = outfile('orig{}.txt'.format(version))\n",
    "combi_file = open('combi{}.txt'.format(version), 'w')\n",
    "\n",
    "stats = collections.Counter()\n",
    "nv = 0\n",
    "nchunk = 1000\n",
    "nvc = 0\n",
    "for v in F.otype.s('verse'):\n",
    "    nv += 1\n",
    "    nvc += 1\n",
    "    if nvc == nchunk:\n",
    "        msg('{:>5} verses {}'.format(nv, stats_prog()))\n",
    "        nvc = 0\n",
    "    passage_label = get_passage(v)\n",
    "    phono_file.write('{}  '.format(passage_label))\n",
    "    orig_file.write('{}  '.format(passage_label))\n",
    "    combi_file.write('{}\\n'.format(passage_label))\n",
    "\n",
    "    words = partition_w(L.d('word', v))\n",
    "    origs = []\n",
    "    phonos = []\n",
    "    \n",
    "    for ws in words:\n",
    "        lws = len(ws)\n",
    "        orig_w = ''.join(get_orig(w, punct=True, set_pet=True, tetra=False) for w in ws)\n",
    "        phono_w = phono(ws, inparts=True, count=True)\n",
    "        origs.append(orig_w)\n",
    "        phonos.extend(phono_w)\n",
    "        for (i, w) in enumerate(ws):\n",
    "            (real_phono, sep) = phono_sep.fullmatch(phono_w[i]).groups()\n",
    "            word_file.write('{}\\t{}\\t{}\\n'.format(w, real_phono, sep))\n",
    "\n",
    "    orig_text = ''.join(origs)\n",
    "    phono_text = ''.join(phonos)\n",
    "    if not phono_text.endswith('.'): word_file.write('{}\\t{}\\t{}\\n'.format('', '', '+'))\n",
    "    orig_file.write('{}\\n'.format(orig_text))\n",
    "    phono_file.write('{}\\n'.format(phono_text))\n",
    "    combi_file.write('{}\\n{}\\n\\n'.format(orig_text, phono_text))\n",
    "\n",
    "phono_file.close()\n",
    "orig_file.close()\n",
    "combi_file.close()\n",
    "word_file.close()\n",
    "msg('{:>5} verses done {}'.format(nv, stats_prog()))\n",
    "for stat in sorted(stats):\n",
    "    amount = stats[stat]\n",
    "    print('{:<1} {:>6} {}'.format(\n",
    "        '#' if amount == 0 else '',\n",
    "        amount,\n",
    "        stat,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency check\n",
    "\n",
    "We take the just generated phono and wordph files.\n",
    "From the phono file we strip the passage indicators, and from the wordph we strip the node numbers.\n",
    "\n",
    "They should be consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2m 58s Reading phono\n",
      " 2m 58s 23213 lines\n",
      " 2m 58s Reading wordph\n",
      " 2m 59s 23213 lines\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: phono text and word info are CONSISTENT\n"
     ]
    }
   ],
   "source": [
    "phono_file = infile(phono_fname)\n",
    "word_file = open(word_fname)\n",
    "\n",
    "strip_passage = re.compile('^\\S+ [0-9]+:[0-9]+\\s*')\n",
    "\n",
    "phono_test = []\n",
    "msg(\"Reading phono\")\n",
    "i = 0\n",
    "for line in phono_file:\n",
    "    i += 1\n",
    "    phono_test.append(strip_passage.sub('', line))\n",
    "phono_file.close()\n",
    "msg('{} lines'.format(i))\n",
    "\n",
    "word_test = []\n",
    "msg(\"Reading wordph\")\n",
    "i = 0\n",
    "for line in word_file:\n",
    "    (mat, sep) = line[0:-1].split('\\t')[1:3]\n",
    "    rsep = '' if sep == '+' else sep\n",
    "    word_test.append(mat+rsep)\n",
    "    if '.' in sep or '+' in sep:\n",
    "        word_test.append('\\n')\n",
    "        i += 1\n",
    "word_file.close()\n",
    "msg('{} lines'.format(i))\n",
    "\n",
    "phono_text = ''.join(phono_test)\n",
    "word_text = ''.join(word_test)\n",
    "if phono_text != word_text:\n",
    "    print('ERROR: phono text and word info are NOT consistent')\n",
    "else:\n",
    "    print('OK: phono text and word info are CONSISTENT')\n",
    "\n",
    "phono_test_name = my_file('phono_x{}.txt'.format(version))\n",
    "word_test_name = my_file('wordph_x{}.txt'.format(version))\n",
    "with open(phono_test_name, 'w') as f: f.write(phono_text)\n",
    "with open(word_test_name, 'w') as f: f.write(word_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
